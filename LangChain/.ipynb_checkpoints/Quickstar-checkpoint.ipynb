{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/g-999/anaconda3/envs/LangChain\n",
      "\n",
      "  added / updated specs:\n",
      "    - langchain\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/linux-64::certifi-2024.2.2-~ --> conda-forge/noarch::certifi-2024.2.2-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install langchain -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo con LangChain\n",
    "LangChain permite desarrollar aplicaciones que conectan fuentes externas de datos y c√°lculos a LLMs (modelos de lenguaje grandes). En este inicio r√°pido, recorreremos algunas formas diferentes de hacerlo. Comenzaremos con una cadena LLM simple, que se basa √∫nicamente en la informaci√≥n en la plantilla de indicaciones para responder. A continuaci√≥n, construiremos una cadena de recuperaci√≥n, que obtiene datos de una base de datos independiente y los pasa a la plantilla de indicaciones. Luego, agregaremos un historial de chat para crear una cadena de recuperaci√≥n de conversaci√≥n. Esto permite interactuar de manera similar a un chat con este LLM, para que recuerde preguntas anteriores. Finalmente, construiremos un agente, que utiliza un LLM para determinar si necesita o no obtener datos para responder preguntas. Abordaremos estos temas a un nivel alto, ¬°pero hay muchos detalles en todo esto! Enlazaremos a documentos relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadena LLM\n",
    "Para esta gu√≠a de inicio, proporcionaremos dos opciones: utilizar OpenAI (un modelo popular disponible a trav√©s de API) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, necesitaremos importar el paquete de integraci√≥n LangChain x OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (0.0.5)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-openai) (0.1.22)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-openai) (1.26.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-openai) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (0.0.87)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayas instalado e inicializado el LLM de tu elecci√≥n, ¬°podemos intentar usarlo! Pregunt√©mosle qu√© es LangSmith: esto es algo que no estaba presente en los datos de entrenamiento, as√≠ que no deber√≠a tener una respuesta muy buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith puede ayudar con las pruebas de varias maneras:\\n\\n1. Automatizaci√≥n de pruebas: Langsmith puede automatizar el proceso de ejecuci√≥n de pruebas, lo que ahorra tiempo y recursos. Puede escribir scripts de prueba y ejecutarlos de manera eficiente para verificar el funcionamiento correcto del software.\\n\\n2. Generaci√≥n de datos de prueba: Langsmith puede generar autom√°ticamente datos de prueba realistas y variados para probar diferentes escenarios y casos de uso. Esto ayuda a aumentar la cobertura de prueba y a identificar posibles problemas o errores.\\n\\n3. Detecci√≥n de errores: Langsmith utiliza t√©cnicas de aprendizaje autom√°tico para analizar los resultados de las pruebas y detectar posibles errores o problemas. Puede identificar patrones o anomal√≠as en los datos de prueba y generar informes detallados sobre los problemas encontrados.\\n\\n4. Optimizaci√≥n de pruebas: Langsmith puede analizar las pruebas existentes y sugerir mejoras o cambios para optimizar el proceso de prueba. Puede identificar pruebas redundantes o innecesarias y ayudar a priorizar las pruebas m√°s cr√≠ticas.\\n\\n5. An√°lisis de c√≥digo: Langsmith puede analizar el c√≥digo fuente del software y proporcionar recomendaciones para mejorar la calidad y la robustez del c√≥digo. Puede identificar posibles problemas de rendimiento, vulnerabilidades de seguridad o malas pr√°cticas de programaci√≥n.\\n\\nEn resumen, Langsmith puede ayudar a agilizar y mejorar el proceso de pruebas, aumentar la cobertura de prueba, detectar errores de manera m√°s eficiente y mejorar la calidad general del software.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"¬øC√≥mo puede ayudar Langsmith con las pruebas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n podemos guiar su respuesta con una plantilla de indicaciones. Las plantillas de indicaciones se utilizan para convertir la entrada de usuario sin procesar en una entrada mejor para el LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un escritor de documentaci√≥n t√©cnica de clase mundial.\"),\n",
    "    (\"user\", \"{entrada}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos combinar estas en una cadena LLM simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocarlo y hacer la misma pregunta. A√∫n no conocer√° la respuesta, pero deber√≠a responder en un tono m√°s adecuado para un escritor t√©cnico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith puede ayudar en las pruebas t√©cnicas de diferentes maneras:\\n\\n1. Documentaci√≥n de pruebas: Langsmith puede crear documentaci√≥n detallada sobre los casos de prueba, los escenarios de prueba y los resultados esperados. Esto ayuda a los equipos de prueba a comprender los requisitos y ejecutar las pruebas de manera efectiva.\\n\\n2. Automatizaci√≥n de pruebas: Langsmith puede escribir scripts de automatizaci√≥n de pruebas utilizando herramientas como Selenium, Appium, JUnit, TestNG, etc. Estos scripts pueden ejecutar pruebas repetitivas de manera r√°pida y precisa, ahorrando tiempo y esfuerzo.\\n\\n3. Documentaci√≥n de pruebas de rendimiento: Langsmith puede escribir documentaci√≥n t√©cnica que describa las pruebas de rendimiento y las m√©tricas a medir. Esto ayuda a los equipos a evaluar el rendimiento de una aplicaci√≥n o sistema y optimizarlo si es necesario.\\n\\n4. An√°lisis de resultados de pruebas: Langsmith puede analizar los resultados de las pruebas y proporcionar informes detallados con recomendaciones para solucionar problemas y mejorar la calidad del software.\\n\\n5. Creaci√≥n de estrategias de prueba: Langsmith puede colaborar con los equipos de desarrollo y QA para definir estrategias de prueba efectivas. Esto incluye identificar los escenarios de prueba clave, las √°reas cr√≠ticas que deben probarse, las t√©cnicas de prueba adecuadas y las mejores pr√°cticas.\\n\\n6. Revisi√≥n de la documentaci√≥n de prueba existente: Langsmith puede revisar y mejorar la documentaci√≥n de prueba existente para asegurarse de que sea completa, precisa y f√°cil de entender.\\n\\nEn resumen, Langsmith es un experto en documentaci√≥n t√©cnica y puede ayudar en todas las fases de las pruebas, desde la planificaci√≥n hasta la ejecuci√≥n y el an√°lisis de resultados.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"entrada\": \"¬øC√≥mo puede ayudar Langsmith con las pruebas?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de un modelo de chat (y, por lo tanto, de esta cadena) es un mensaje. Sin embargo, a menudo es mucho m√°s conveniente trabajar con cadenas. A√±adamos un simple analizador de salida para convertir el mensaje de chat en una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos agregar esto a la cadena anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocarlo y hacer la misma pregunta. La respuesta ahora ser√° una cadena (en lugar de un ChatMessage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como escritor de documentaci√≥n t√©cnica, Langsmith puede ayudar en las pruebas de varias formas:\\n\\n1. Creaci√≥n de casos de prueba: Langsmith puede colaborar con los equipos de desarrollo y pruebas para entender los requerimientos y funcionalidades del software. Con esta informaci√≥n, puede crear casos de prueba claros y detallados que ayuden a los equipos de pruebas a validar el correcto funcionamiento del software.\\n\\n2. Documentaci√≥n de los resultados de las pruebas: Una vez que los equipos de pruebas ejecuten los casos de prueba, Langsmith puede documentar los resultados de manera clara y concisa. Esto incluye registrar los errores encontrados, su gravedad y c√≥mo reproducirlos, as√≠ como tambi√©n documentar los casos de prueba exitosos.\\n\\n3. Creaci√≥n de gu√≠as de pruebas: Langsmith puede escribir gu√≠as de pruebas detalladas que orienten a los equipos de pruebas sobre c√≥mo ejecutar los casos de prueba correctamente. Estas gu√≠as pueden incluir informaci√≥n sobre los pasos a seguir, los datos de prueba necesarios y los resultados esperados.\\n\\n4. Revisi√≥n de documentaci√≥n de pruebas existente: Si hay documentaci√≥n de pruebas existente, Langsmith puede revisarla y asegurarse de que est√© completa, precisa y f√°cil de entender. Esto puede incluir la revisi√≥n de casos de prueba, gu√≠as de pruebas, informes de errores y cualquier otra documentaci√≥n relacionada con las pruebas.\\n\\n5. Colaboraci√≥n con equipos de pruebas: Langsmith puede colaborar estrechamente con los equipos de pruebas para entender sus necesidades y desaf√≠os, y proporcionarles orientaci√≥n y soporte continuo. Esto puede incluir la participaci√≥n en reuniones de planificaci√≥n de pruebas, revisiones de dise√±o y pruebas de aceptaci√≥n.\\n\\nEn resumen, como escritor de documentaci√≥n t√©cnica, Langsmith puede ayudar en las pruebas proporcionando casos de prueba detallados, documentando los resultados de las pruebas, creando gu√≠as de pruebas, revisando la documentaci√≥n existente y colaborando estrechamente con los equipos de pruebas. Su experiencia en la redacci√≥n t√©cnica le permite comunicar de manera clara y efectiva los aspectos necesarios para llevar a cabo pruebas exitosas.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"entrada\": \"¬øC√≥mo puede ayudar Langsmith con las pruebas?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profundizando‚Äã\n",
    "Ahora hemos configurado con √©xito una cadena LLM b√°sica. Solo hemos explorado los conceptos b√°sicos de indicaciones, modelos y analizadores de salida; para obtener una comprensi√≥n m√°s profunda de todo lo mencionado aqu√≠, consulta esta secci√≥n de la documentaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadena de Recuperaci√≥n\n",
    "Para responder adecuadamente a la pregunta original (\"¬øc√≥mo puede ayudar LangSmith con las pruebas?\"), necesitamos proporcionar contexto adicional al LLM. Podemos hacer esto mediante la recuperaci√≥n. La recuperaci√≥n es √∫til cuando tienes demasiados datos para pasar al LLM directamente. Luego puedes usar un recuperador para buscar solo las piezas m√°s relevantes y pasarlas.\n",
    "\n",
    "En este proceso, buscaremos documentos relevantes desde un Recuperador y luego los pasaremos a la indicaci√≥n. Un Recuperador puede basarse en cualquier cosa, como una tabla SQL, internet, etc., pero en este caso poblaremos una tienda de vectores y la utilizaremos como un recuperador. Para obtener m√°s informaci√≥n sobre las tiendas de vectores, consulta esta documentaci√≥n.\n",
    "\n",
    "Primero, necesitamos cargar los datos que queremos indexar. Para hacer esto, utilizaremos el WebBaseLoader. Esto requiere instalar BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/g-999/anaconda3/envs/LangChain\n",
      "\n",
      "  added / updated specs:\n",
      "    - beautifulsoup4\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2024.2.2-~ --> pkgs/main/linux-64::certifi-2024.2.2-py311h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despu√©s de eso, podemos importar y usar WebBaseLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, necesitamos indexarlo en una tienda de vectores. Esto requiere algunos componentes, en particular, un modelo de incrustaci√≥n y una tienda de vectores.\n",
    "\n",
    "En cuanto a los modelos de incrustaci√≥n, proporcionamos nuevamente ejemplos de acceso a trav√©s de OpenAI o mediante modelos locales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aseg√∫rate de tener instalado el paquete `langchain_openai` y las variables de entorno adecuadas configuradas (son las mismas que se necesitan para el LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos utilizar este modelo de incrustaci√≥n para ingresar documentos en una tienda de vectores. Utilizaremos una tienda de vectores local y sencilla, FAISS, por simplicidad.\n",
    "\n",
    "Primero, necesitamos instalar los paquetes necesarios para ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos construir nuestro √≠ndice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos estos datos indexados en una tienda de vectores, crearemos una cadena de recuperaci√≥n. Esta cadena tomar√° una pregunta entrante, buscar√° documentos relevantes y luego pasar√° esos documentos junto con la pregunta original a un LLM y le pedir√° que responda la pregunta original.\n",
    "\n",
    "Primero, configuremos la cadena que toma una pregunta y los documentos recuperados y genera una respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Responda la siguiente pregunta bas√°ndose √∫nicamente en el contexto proporcionado:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith puede permitirle visualizar los resultados de las pruebas.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"¬øC√≥mo puede ayudar Langsmith con las pruebas?\",\n",
    "    \"context\": [Document(page_content=\"Langsmith puede permitirle visualizar los resultados de las pruebas\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, queremos que los documentos provengan primero del recuperador que acabamos de configurar. De esta manera, para una pregunta dada, podemos utilizar el recuperador para seleccionar din√°micamente los documentos m√°s relevantes y pasarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocar esta cadena. Esto devuelve un diccionario: la respuesta del LLM se encuentra en la clave \"answer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith puede ayudar con las pruebas de varias formas. \n",
      "\n",
      "En primer lugar, LangSmith proporciona una visualizaci√≥n clara de las entradas y salidas exactas de todas las llamadas al modelo de lenguaje (LLM). Esto permite comprender f√°cilmente c√≥mo se est√°n procesando los datos y facilita la depuraci√≥n de posibles errores en la l√≥gica de formato o transformaciones inesperadas de las entradas del usuario.\n",
      "\n",
      "Adem√°s, LangSmith ofrece una funci√≥n de \"Playground\" que permite editar la entrada de una llamada al LLM y ver c√≥mo afecta la salida. Esto es √∫til para probar diferentes cambios y observar los resultados sin tener que copiar y pegar la entrada en un entorno separado.\n",
      "\n",
      "LangSmith tambi√©n simplifica la carga de conjuntos de datos para probar cambios en las cadenas de comandos o en las respuestas del modelo. Permite ejecutar la cadena de comandos en los puntos de datos del conjunto de datos y visualizar los resultados. Tambi√©n proporciona herramientas para asignar comentarios y marcar las ejecuciones como correctas o incorrectas.\n",
      "\n",
      "Adem√°s, LangSmith ofrece evaluadores autom√°ticos y una funci√≥n de evaluaci√≥n humana para evaluar la calidad y el comportamiento del modelo. Los evaluadores autom√°ticos pueden ayudar a identificar ejemplos que deben revisarse y la evaluaci√≥n humana permite una revisi√≥n manual m√°s detallada y la validaci√≥n de los resultados autom√°ticos.\n",
      "\n",
      "En resumen, LangSmith ayuda en las pruebas proporcionando visibilidad y herramientas para comprender y depurar las entradas y salidas del modelo, probar diferentes cambios y evaluar la calidad del modelo tanto de forma autom√°tica como manual.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"¬øC√≥mo puede ayudar Langsmith con las pruebas?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadena de Recuperaci√≥n de Conversaci√≥n\n",
    "La cadena que hemos creado hasta ahora solo puede responder preguntas individuales. Uno de los principales tipos de aplicaciones de LLM que las personas est√°n desarrollando son los chatbots. Entonces, ¬øc√≥mo convertimos esta cadena en una que pueda responder preguntas de seguimiento?\n",
    "\n",
    "Todav√≠a podemos utilizar la funci√≥n create_retrieval_chain, pero necesitamos cambiar dos cosas:\n",
    "\n",
    "1. El m√©todo de recuperaci√≥n ahora no solo debe funcionar en la entrada m√°s reciente, sino que debe tener en cuenta toda la historia.\n",
    "2. La cadena final del LLM tambi√©n debe tener en cuenta toda la historia.\n",
    "\n",
    "Actualizaci√≥n de la Recuperaci√≥n\n",
    "\n",
    "Para actualizar la recuperaci√≥n, crearemos una nueva cadena. Esta cadena tomar√° la entrada m√°s reciente (input) y el historial de la conversaci√≥n (chat_history) y utilizar√° un LLM para generar una consulta de b√∫squeda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Dada la conversaci√≥n anterior, genere una consulta de b√∫squeda para buscar informaci√≥n relevante para la conversaci√≥n.\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar esto pasando una instancia en la que el usuario hace una pregunta de seguimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith‚Äôs tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don‚Äôt even look at the traces, but the 10% of the time that we do‚Ä¶ it‚Äôs so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string ‚Üí string (or chat messages ‚Üí chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî mirroring the debug mode approach.We‚Äôve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren‚Äôt just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.‚Ü©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a ‚ÄúShare‚Äù button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, it‚Äôs often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You don‚Äôt have any examples to benchmark your changes against.LangSmith addresses this problem by including an ‚ÄúAdd to Dataset‚Äù button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"¬øPuede LangSmith ayudarme a probar mis solicitudes de LLM?\"), AIMessage(content=\"¬°Si!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Dime como\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deber√≠as ver que esto devuelve documentos sobre pruebas en LangSmith. Esto se debe a que el LLM gener√≥ una nueva consulta, combinando el historial del chat con la pregunta de seguimiento.\n",
    "\n",
    "Ahora que tenemos este nuevo recuperador, podemos crear una nueva cadena para continuar la conversaci√≥n teniendo en cuenta estos documentos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responda las preguntas del usuario seg√∫n el siguiente contexto:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='¬øPuede LangSmith ayudarme a probar mis solicitudes de LLM?'),\n",
       "  AIMessage(content='¬°SI!')],\n",
       " 'input': 'Dime como',\n",
       " 'context': [Document(page_content=\"Skip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith‚Äôs tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don‚Äôt even look at the traces, but the 10% of the time that we do‚Ä¶ it‚Äôs so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string ‚Üí string (or chat messages ‚Üí chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî mirroring the debug mode approach.We‚Äôve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren‚Äôt just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.‚Ü©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a ‚ÄúShare‚Äù button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, it‚Äôs often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You don‚Äôt have any examples to benchmark your changes against.LangSmith addresses this problem by including an ‚ÄúAdd to Dataset‚Äù button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith puede ayudarte a probar tus solicitudes de LLM de varias maneras:\\n\\n1. Visualizaci√≥n de entradas y salidas exactas: LangSmith proporciona una visualizaci√≥n clara de las entradas y salidas exactas de todas las llamadas de LLM. Esto te permite comprender f√°cilmente qu√© se est√° enviando al modelo y qu√© est√° devolviendo como resultado.\\n\\n2. Modificaci√≥n de la solicitud en tiempo real: Cuando examinas una llamada de LLM en LangSmith, puedes hacer clic en el bot√≥n \"Abrir en Playground\" para acceder a un entorno de prueba. Aqu√≠ puedes modificar la solicitud y volver a ejecutarla para observar los cambios en el resultado. Esto te permite experimentar y probar diferentes variaciones de la solicitud para ver c√≥mo afectan al resultado.\\n\\n3. Agregar ejemplos a conjuntos de datos: LangSmith te permite agregar f√°cilmente ejemplos de entrada/salida a conjuntos de datos. Puedes editar estos ejemplos antes de agregarlos al conjunto de datos para incluir el resultado esperado. Esto es especialmente √∫til para crear conjuntos de datos de prueba con ejemplos positivos y negativos.\\n\\n4. Evaluaci√≥n manual y ad hoc: LangSmith te permite evaluar manualmente tus solicitudes de LLM y analizar los resultados. Puedes comparar los resultados obtenidos con los resultados esperados y realizar ajustes en tus solicitudes en funci√≥n de los resultados obtenidos.\\n\\nEn resumen, LangSmith proporciona herramientas y funcionalidades que te permiten probar y evaluar tus solicitudes de LLM de manera efectiva, lo que te ayuda a mejorar la calidad y el rendimiento de tus modelos.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"¬øPuede LangSmith ayudarme a probar mis solicitudes de LLM?\"), AIMessage(content=\"¬°SI!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Dime como\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que esto da una respuesta coherente: ¬°hemos convertido con √©xito nuestra cadena de recuperaci√≥n en un chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente\n",
    "Hasta ahora, hemos creado ejemplos de cadenas, donde cada paso se conoce de antemano. Lo √∫ltimo que crearemos es un agente, donde el LLM decide qu√© pasos tomar.\n",
    "\n",
    "NOTA: para este ejemplo, solo mostraremos c√≥mo crear un agente utilizando modelos de OpenAI, ya que los modelos locales a√∫n no son lo suficientemente confiables.\n",
    "\n",
    "Una de las primeras cosas que debes hacer al construir un agente es decidir a qu√© herramientas deber√≠a tener acceso. Para este ejemplo, daremos al agente acceso a dos herramientas:\n",
    "\n",
    "1. El recuperador que acabamos de crear. Esto le permitir√° responder f√°cilmente preguntas sobre LangSmith.\n",
    "2. Una herramienta de b√∫squeda. Esto le permitir√° responder f√°cilmente preguntas que requieran informaci√≥n actualizada.\n",
    "\n",
    "Primero, configuremos una herramienta para el recuperador que acabamos de crear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Buscar informaci√≥n sobre LangSmith. ¬°Para cualquier pregunta sobre LangSmith, debes utilizar esta herramienta!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La herramienta de b√∫squeda que utilizaremos es Tavily. Esto requerir√° una clave API (tienen un generoso nivel gratuito). Despu√©s de crearlo en su plataforma, debes configurarlo como una variable de entorno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualizamos el .env con la clave de tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear una lista de las herramientas con las que queremos trabajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las herramientas, podemos crear un agente para usarlas. Repasaremos esto r√°pidamente; para profundizar en lo que est√° sucediendo exactamente, consulte la documentaci√≥n de introducci√≥n del agente. \n",
    "\n",
    "Instale primero el HUB langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Downloading types_requests-2.31.0.20240125-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20240125\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usarlo para obtener un mensaje predefinido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Ahora podemos invocar al agente y ver c√≥mo responde! Podemos hacerle preguntas sobre LangSmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith puede ayudar con las pruebas de varias maneras:\n",
      "\n",
      "1. Generaci√≥n autom√°tica de casos de prueba: LangSmith puede generar autom√°ticamente casos de prueba para diferentes escenarios y condiciones. Esto ahorra tiempo y esfuerzo al crear casos de prueba manualmente.\n",
      "\n",
      "2. Ejecuci√≥n de pruebas automatizadas: LangSmith puede ejecutar pruebas automatizadas para verificar el funcionamiento correcto de una aplicaci√≥n o sistema. Esto incluye la ejecuci√≥n de pruebas de regresi√≥n, pruebas de integraci√≥n y pruebas de rendimiento.\n",
      "\n",
      "3. An√°lisis de resultados de pruebas: LangSmith puede analizar los resultados de las pruebas y proporcionar informes detallados sobre los errores encontrados, el rendimiento del sistema y la cobertura de las pruebas.\n",
      "\n",
      "4. Optimizaci√≥n de pruebas: LangSmith puede ayudar a optimizar las pruebas identificando √°reas cr√≠ticas que requieren una mayor cobertura de pruebas y sugiriendo mejoras en los casos de prueba existentes.\n",
      "\n",
      "5. Integraci√≥n con herramientas de gesti√≥n de pruebas: LangSmith se puede integrar con herramientas de gesti√≥n de pruebas existentes para facilitar la planificaci√≥n, seguimiento y gesti√≥n de las pruebas.\n",
      "\n",
      "En resumen, LangSmith puede automatizar y optimizar el proceso de pruebas, lo que ayuda a garantizar la calidad y confiabilidad de las aplicaciones y sistemas.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¬øC√≥mo puede ayudar Langsmith con las pruebas?',\n",
       " 'output': 'LangSmith puede ayudar con las pruebas de varias maneras:\\n\\n1. Generaci√≥n autom√°tica de casos de prueba: LangSmith puede generar autom√°ticamente casos de prueba para diferentes escenarios y condiciones. Esto ahorra tiempo y esfuerzo al crear casos de prueba manualmente.\\n\\n2. Ejecuci√≥n de pruebas automatizadas: LangSmith puede ejecutar pruebas automatizadas para verificar el funcionamiento correcto de una aplicaci√≥n o sistema. Esto incluye la ejecuci√≥n de pruebas de regresi√≥n, pruebas de integraci√≥n y pruebas de rendimiento.\\n\\n3. An√°lisis de resultados de pruebas: LangSmith puede analizar los resultados de las pruebas y proporcionar informes detallados sobre los errores encontrados, el rendimiento del sistema y la cobertura de las pruebas.\\n\\n4. Optimizaci√≥n de pruebas: LangSmith puede ayudar a optimizar las pruebas identificando √°reas cr√≠ticas que requieren una mayor cobertura de pruebas y sugiriendo mejoras en los casos de prueba existentes.\\n\\n5. Integraci√≥n con herramientas de gesti√≥n de pruebas: LangSmith se puede integrar con herramientas de gesti√≥n de pruebas existentes para facilitar la planificaci√≥n, seguimiento y gesti√≥n de las pruebas.\\n\\nEn resumen, LangSmith puede automatizar y optimizar el proceso de pruebas, lo que ayuda a garantizar la calidad y confiabilidad de las aplicaciones y sistemas.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"¬øC√≥mo puede ayudar Langsmith con las pruebas?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos preguntarle sobre el clima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'clima en NY'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://weatherspark.com/h/y/23912/2024/Historical-Weather-during-2024-in-New-York-City-New-York-United-States', 'content': '2024 Weather History in New York City New York, United States  New York City Temperature History 2024 Hourly Temperature in 2024 in New York City  Daily Precipitation in 2024 in New York City Observed Weather in 2024 in New York City  Humidity Comfort Levels in 2024 in New York City Wind Speed in 2024 in New York CityLatest Report ‚Äî 10:51 AM Fri, Feb 2, 2024 17 min ago UTC 15:51 Call Sign KEWR Temp. 42.1 ¬∞F very cold Precipitation - last 1 hr 0.02 in Wind 11.5 mph gentle breeze Wind Dir. 360 deg, N Cloud Cover Overcast 3,000 ft Mostly Cloudy 700 ft Mostly Cloudy 1,500 ft'}]\u001b[0m\u001b[32;1m\u001b[1;3mSeg√∫n los √∫ltimos informes, el clima en Nueva York es de 42.1 ¬∞F (5.6 ¬∞C) con viento de 11.5 mph (18.5 km/h) en direcci√≥n norte. La cobertura de nubes es mayormente nublado con algunas nubes a 3,000 pies y 700 pies.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¬øCu√°l es el clima en NY?',\n",
       " 'output': 'Seg√∫n los √∫ltimos informes, el clima en Nueva York es de 42.1 ¬∞F (5.6 ¬∞C) con viento de 11.5 mph (18.5 km/h) en direcci√≥n norte. La cobertura de nubes es mayormente nublado con algunas nubes a 3,000 pies y 700 pies.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"¬øCu√°l es el clima en NY?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mPara probar tus solicitudes de LLM (Lenguaje de Modelado de Lenguajes) con LangSmith, puedes seguir estos pasos:\n",
      "\n",
      "1. Define tu solicitud de LLM: Primero, debes tener claro qu√© tipo de solicitud de LLM quieres probar. Puedes especificar el tipo de modelo de lenguaje que deseas utilizar, las entradas y salidas esperadas, y cualquier otra informaci√≥n relevante.\n",
      "\n",
      "2. Utiliza la herramienta de b√∫squeda de LangSmith: Puedes utilizar la funci√≥n `langsmith_search` para buscar informaci√≥n sobre LangSmith y obtener respuestas a tus preguntas espec√≠ficas. Esta funci√≥n te proporcionar√° informaci√≥n detallada sobre c√≥mo utilizar LangSmith para probar tus solicitudes de LLM.\n",
      "\n",
      "3. Consulta la documentaci√≥n de LangSmith: LangSmith proporciona documentaci√≥n detallada sobre c√≥mo utilizar su plataforma para probar solicitudes de LLM. Puedes acceder a esta documentaci√≥n en su sitio web oficial o a trav√©s de la herramienta de b√∫squeda de LangSmith.\n",
      "\n",
      "4. Prueba tus solicitudes de LLM: Una vez que hayas comprendido c√≥mo utilizar LangSmith, puedes comenzar a probar tus solicitudes de LLM. Puedes enviar tus solicitudes a LangSmith y analizar las respuestas generadas por el modelo de lenguaje.\n",
      "\n",
      "Recuerda que LangSmith es una herramienta poderosa y vers√°til para probar solicitudes de LLM, pero es importante tener en cuenta las limitaciones y consideraciones espec√≠ficas de cada caso de uso. Si tienes alguna pregunta adicional o necesitas ayuda espec√≠fica, no dudes en preguntar.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='¬øPuede LangSmith ayudarme a probar mis solicitudes de LLM?'),\n",
       "  AIMessage(content='¬°Si!')],\n",
       " 'input': 'Dime como',\n",
       " 'output': 'Para probar tus solicitudes de LLM (Lenguaje de Modelado de Lenguajes) con LangSmith, puedes seguir estos pasos:\\n\\n1. Define tu solicitud de LLM: Primero, debes tener claro qu√© tipo de solicitud de LLM quieres probar. Puedes especificar el tipo de modelo de lenguaje que deseas utilizar, las entradas y salidas esperadas, y cualquier otra informaci√≥n relevante.\\n\\n2. Utiliza la herramienta de b√∫squeda de LangSmith: Puedes utilizar la funci√≥n `langsmith_search` para buscar informaci√≥n sobre LangSmith y obtener respuestas a tus preguntas espec√≠ficas. Esta funci√≥n te proporcionar√° informaci√≥n detallada sobre c√≥mo utilizar LangSmith para probar tus solicitudes de LLM.\\n\\n3. Consulta la documentaci√≥n de LangSmith: LangSmith proporciona documentaci√≥n detallada sobre c√≥mo utilizar su plataforma para probar solicitudes de LLM. Puedes acceder a esta documentaci√≥n en su sitio web oficial o a trav√©s de la herramienta de b√∫squeda de LangSmith.\\n\\n4. Prueba tus solicitudes de LLM: Una vez que hayas comprendido c√≥mo utilizar LangSmith, puedes comenzar a probar tus solicitudes de LLM. Puedes enviar tus solicitudes a LangSmith y analizar las respuestas generadas por el modelo de lenguaje.\\n\\nRecuerda que LangSmith es una herramienta poderosa y vers√°til para probar solicitudes de LLM, pero es importante tener en cuenta las limitaciones y consideraciones espec√≠ficas de cada caso de uso. Si tienes alguna pregunta adicional o necesitas ayuda espec√≠fica, no dudes en preguntar.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"¬øPuede LangSmith ayudarme a probar mis solicitudes de LLM?\"), AIMessage(content=\"¬°Si!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Dime como\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sirviendo con LangServe\n",
    "Ahora que hemos construido una aplicaci√≥n, necesitamos ponerla en funcionamiento. Ah√≠ es donde entra LangServe. LangServe ayuda a los desarrolladores a implementar cadenas de LangChain como una API REST. No es necesario utilizar LangServe para usar LangChain, pero en esta gu√≠a mostraremos c√≥mo puedes implementar tu aplicaci√≥n con LangServe.\n",
    "\n",
    "Mientras que la primera parte de esta gu√≠a estaba destinada a ejecutarse en un cuaderno Jupyter, ahora saldremos de eso. Crearemos un archivo Python y luego interactuaremos con √©l desde la l√≠nea de comandos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langserve[all]\n",
      "  Downloading langserve-0.0.41-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting fastapi<1,>=0.90.1 (from langserve[all])\n",
      "  Downloading fastapi-0.109.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (0.26.0)\n",
      "Collecting httpx-sse>=0.3.1 (from langserve[all])\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain>=0.0.333 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (0.1.6)\n",
      "Collecting orjson>=2 (from langserve[all])\n",
      "  Downloading orjson-3.9.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (1.10.12)\n",
      "Collecting sse-starlette<2.0.0,>=1.3.0 (from langserve[all])\n",
      "  Downloading sse_starlette-1.8.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting starlette<0.37.0,>=0.36.3 (from fastapi<1,>=0.90.1->langserve[all])\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (4.9.0)\n",
      "Requirement already satisfied: anyio in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (4.2.0)\n",
      "Requirement already satisfied: certifi in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->langserve[all]) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.19)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.1.22)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (1.26.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (8.2.3)\n",
      "Collecting uvicorn (from sse-starlette<2.0.0,>=1.3.0->langserve[all])\n",
      "  Downloading uvicorn-0.27.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (3.20.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.333->langserve[all]) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.22->langchain>=0.0.333->langserve[all]) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.333->langserve[all]) (3.0.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from uvicorn->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (1.0.0)\n",
      "Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading orjson-3.9.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n",
      "Downloading langserve-0.0.41-py3-none-any.whl (512 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.8/512.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uvicorn, orjson, httpx-sse, starlette, fastapi, sse-starlette, langserve\n",
      "Successfully installed fastapi-0.109.2 httpx-sse-0.4.0 langserve-0.0.41 orjson-3.9.13 sse-starlette-1.8.2 starlette-0.36.3 uvicorn-0.27.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Servidor \n",
    "Para crear un servidor para nuestra aplicaci√≥n, crearemos un archivo [serve.py](http://localhost:8889/lab/tree/Documentos/GitClone/CookBook/LangChain/serve.py). \n",
    "Esto contendr√° nuestra l√≥gica para servir nuestra aplicaci√≥n. \n",
    "Consta de tres cosas: \n",
    "1. La definici√≥n de nuestra cadena que acabamos de crear arriba\n",
    "2. Nuestra aplicaci√≥n FastAPI\n",
    "3. Una definici√≥n de una ruta desde la cual servir a la cadena, que se hace con langserve.add_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deber√≠amos ver nuestra [cadena](http://localhost:8889/lab/tree/Documentos/GitClone/CookBook/LangChain/serve.py) siendo atendida en localhost:8000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground \n",
    "Cada servicio LangServe viene con una interfaz de usuario integrada simple para configurar e invocar la aplicaci√≥n con salida de transmisi√≥n y visibilidad de los pasos intermedios. \n",
    "Dir√≠gete a http://localhost:8000/agent/playground/ ¬°para probarlo! Pase la misma pregunta que antes: \"¬øC√≥mo puede ayudar Langsmith con las pruebas?\" - y deber√≠a responder igual que antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliente \n",
    "Ahora configuremos un cliente para interactuar mediante programaci√≥n con nuestro servicio. Podemos hacer esto f√°cilmente con [langserve.RemoteRunnable](/docs/langserve#client) Al usar esto, podemos interactuar con la cadena servida como si se estuviera ejecutando en el lado del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'LangSmith puede ayudar con las pruebas de varias maneras:\\n\\n1. Generaci√≥n autom√°tica de casos de prueba: LangSmith puede generar autom√°ticamente casos de prueba basados en las especificaciones y requisitos del software. Esto ahorra tiempo y esfuerzo en la creaci√≥n manual de casos de prueba.\\n\\n2. Ejecuci√≥n de pruebas automatizadas: LangSmith puede ejecutar pruebas automatizadas para verificar el funcionamiento correcto del software. Esto incluye pruebas de regresi√≥n, pruebas de integraci√≥n y pruebas de unidad.\\n\\n3. An√°lisis de cobertura de c√≥digo: LangSmith puede analizar la cobertura de c√≥digo para identificar √°reas no probadas del software. Esto ayuda a garantizar una cobertura completa de las pruebas y a identificar posibles problemas o errores.\\n\\n4. Generaci√≥n de informes de pruebas: LangSmith puede generar informes detallados de las pruebas realizadas, incluyendo resultados, errores encontrados y m√©tricas de calidad. Esto facilita el seguimiento y la gesti√≥n de las pruebas.\\n\\n5. Integraci√≥n con herramientas de gesti√≥n de pruebas: LangSmith puede integrarse con herramientas de gesti√≥n de pruebas existentes, como JIRA o TestRail, para facilitar la planificaci√≥n, seguimiento y gesti√≥n de las pruebas.\\n\\nEn resumen, LangSmith puede automatizar y optimizar el proceso de pruebas, ayudando a garantizar la calidad y fiabilidad del software.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
    "remote_chain.invoke({\n",
    "    \"input\": \"¬øC√≥mo puede ayudar Langsmith con las pruebas?\",\n",
    "    \"chat_history\": []  # Providing an empty list as this is the first call\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
