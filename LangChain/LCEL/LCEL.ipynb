{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14427219-f02c-4399-ad13-fcb307c6e4e4",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "El Lenguaje de Expresión de Cadenas de LangChain, o LCEL, es una manera declarativa de componer fácilmente cadenas juntas. El LCEL fue diseñado desde el primer día para admitir la implementación de prototipos en producción, sin cambios de código, desde la cadena más simple de \"prompt + LLM\" hasta las cadenas más complejas (hemos visto a personas ejecutar con éxito cadenas LCEL con 100s de pasos en producción). Para resaltar algunas de las razones por las que podrías querer usar LCEL:\n",
    "\n",
    "Soporte de transmisión: al construir tus cadenas con LCEL, obtienes el mejor tiempo posible hasta el primer token (tiempo transcurrido hasta que sale el primer fragmento de salida). Para algunas cadenas, esto significa, por ejemplo, transmitir tokens directamente desde un LLM a un analizador de salida en continuo, y obtener fragmentos analizados e incrementales de salida a la misma velocidad que el proveedor de LLM emite los tokens en bruto.\n",
    "\n",
    "Soporte asíncrono: cualquier cadena construida con LCEL puede ser llamada tanto con la API síncrona (por ejemplo, en tu cuaderno Jupyter durante la prototipificación) como con la API asíncrona (por ejemplo, en un servidor LangServe). Esto permite utilizar el mismo código para prototipos y en producción, con un rendimiento excelente y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
    "\n",
    "Ejecución paralela optimizada: siempre que las cadenas LCEL tengan pasos que se puedan ejecutar en paralelo (por ejemplo, si recuperas documentos de varios recuperadores), lo hacemos automáticamente, tanto en las interfaces síncronas como asíncronas, para la latencia más pequeña posible.\n",
    "\n",
    "Reintentos y alternativas: configura reintentos y alternativas para cualquier parte de tu cadena LCEL. Esta es una excelente manera de hacer que tus cadenas sean más confiables a escala. Actualmente estamos trabajando en agregar soporte de transmisión para reintentos/alternativas, para que puedas obtener confiabilidad adicional sin costo de latencia.\n",
    "\n",
    "Acceso a resultados intermedios: para cadenas más complejas, a menudo es muy útil acceder a los resultados de pasos intermedios incluso antes de que se produzca la salida final. Esto se puede usar para informar a los usuarios finales que algo está sucediendo, o incluso solo para depurar tu cadena. Puedes transmitir resultados intermedios, y está disponible en cada servidor LangServe.\n",
    "\n",
    "Esquemas de entrada y salida: los esquemas de entrada y salida proporcionan a cada cadena LCEL esquemas Pydantic y JSONSchema inferidos a partir de la estructura de tu cadena. Esto se puede usar para la validación de entradas y salidas, y es una parte integral de LangServe.\n",
    "\n",
    "Integración transparente con el seguimiento de LangSmith: a medida que tus cadenas se vuelven más y más complejas, es cada vez más importante entender qué está sucediendo exactamente en cada paso. Con LCEL, todos los pasos se registran automáticamente en LangSmith para una máxima observabilidad y capacidad de depuración.\n",
    "\n",
    "Integración transparente con el despliegue de LangServe: cualquier cadena creada con LCEL se puede implementar fácilmente utilizando LangServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f1163-0f20-49c5-ba03-aaa54b0dc20c",
   "metadata": {},
   "source": [
    "# Comenzar\n",
    "LCEL facilita la construcción de cadenas complejas a partir de componentes básicos y admite funcionalidades listas para usar, como transmisión, paralelismo y registro.\n",
    "\n",
    "## Ejemplo básico: prompt + modelo + analizador de salida\n",
    "El caso de uso más básico y común es encadenar una plantilla de prompt y un modelo. Para ver cómo funciona esto, creemos una cadena que tome un tema y genere un chiste:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfb6775-2324-4276-8cfd-95cea6eb0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-core langchain-community langchain-openai\n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19dedebc-9f6d-48a4-a690-778c05e812e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Sabes cuál es el chiste favorito de los procrastinadores?\\n\\nMañana te lo cuento.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Cuentame un chiste {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"procrastinación\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc88d33-c123-4652-bca7-1370f42a73da",
   "metadata": {},
   "source": [
    "Observa esta línea de código, donde ensamblamos diferentes componentes en una única cadena utilizando LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79a74f8-f65f-400d-9c2d-ae631ce8f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b64817-3205-462e-92e4-86cac31c3eaa",
   "metadata": {},
   "source": [
    "El símbolo | es similar a un operador de tubería de Unix, que encadena los diferentes componentes alimentando la salida de un componente como entrada al siguiente componente.\n",
    "\n",
    "En esta cadena, la entrada del usuario se pasa a la plantilla de prompt, luego la salida de la plantilla de prompt se pasa al modelo, y finalmente, la salida del modelo se pasa al analizador de salida. Echemos un vistazo a cada componente individualmente para entender realmente lo que está sucediendo.\n",
    "\n",
    "## 1.  Prompt\n",
    "`prompt` es un `BasePromptTemplate`, lo que significa que toma un diccionario de variables de plantilla y produce un `PromptValue`. Un `PromptValue` es un contenedor alrededor de un prompt completado que se puede pasar tanto a un LLM (que toma una cadena como entrada) como a un ChatModel (que toma una secuencia de mensajes como entrada). Puede trabajar con ambos tipos de modelos de lenguaje porque define lógica tanto para producir `BaseMessages` como para producir una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ab435-f519-44cc-8dbc-8ff258a721b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
