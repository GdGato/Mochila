{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14427219-f02c-4399-ad13-fcb307c6e4e4",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "El Lenguaje de Expresión de Cadenas de LangChain, o LCEL, es una manera declarativa de componer fácilmente cadenas juntas. El LCEL fue diseñado desde el primer día para admitir la implementación de prototipos en producción, sin cambios de código, desde la cadena más simple de \"prompt + LLM\" hasta las cadenas más complejas (hemos visto a personas ejecutar con éxito cadenas LCEL con 100s de pasos en producción). Para resaltar algunas de las razones por las que podrías querer usar LCEL:\n",
    "\n",
    "Soporte de transmisión: al construir tus cadenas con LCEL, obtienes el mejor tiempo posible hasta el primer token (tiempo transcurrido hasta que sale el primer fragmento de salida). Para algunas cadenas, esto significa, por ejemplo, transmitir tokens directamente desde un LLM a un analizador de salida en continuo, y obtener fragmentos analizados e incrementales de salida a la misma velocidad que el proveedor de LLM emite los tokens en bruto.\n",
    "\n",
    "Soporte asíncrono: cualquier cadena construida con LCEL puede ser llamada tanto con la API síncrona (por ejemplo, en tu cuaderno Jupyter durante la prototipificación) como con la API asíncrona (por ejemplo, en un servidor LangServe). Esto permite utilizar el mismo código para prototipos y en producción, con un rendimiento excelente y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
    "\n",
    "Ejecución paralela optimizada: siempre que las cadenas LCEL tengan pasos que se puedan ejecutar en paralelo (por ejemplo, si recuperas documentos de varios recuperadores), lo hacemos automáticamente, tanto en las interfaces síncronas como asíncronas, para la latencia más pequeña posible.\n",
    "\n",
    "Reintentos y alternativas: configura reintentos y alternativas para cualquier parte de tu cadena LCEL. Esta es una excelente manera de hacer que tus cadenas sean más confiables a escala. Actualmente estamos trabajando en agregar soporte de transmisión para reintentos/alternativas, para que puedas obtener confiabilidad adicional sin costo de latencia.\n",
    "\n",
    "Acceso a resultados intermedios: para cadenas más complejas, a menudo es muy útil acceder a los resultados de pasos intermedios incluso antes de que se produzca la salida final. Esto se puede usar para informar a los usuarios finales que algo está sucediendo, o incluso solo para depurar tu cadena. Puedes transmitir resultados intermedios, y está disponible en cada servidor LangServe.\n",
    "\n",
    "Esquemas de entrada y salida: los esquemas de entrada y salida proporcionan a cada cadena LCEL esquemas Pydantic y JSONSchema inferidos a partir de la estructura de tu cadena. Esto se puede usar para la validación de entradas y salidas, y es una parte integral de LangServe.\n",
    "\n",
    "Integración transparente con el seguimiento de LangSmith: a medida que tus cadenas se vuelven más y más complejas, es cada vez más importante entender qué está sucediendo exactamente en cada paso. Con LCEL, todos los pasos se registran automáticamente en LangSmith para una máxima observabilidad y capacidad de depuración.\n",
    "\n",
    "Integración transparente con el despliegue de LangServe: cualquier cadena creada con LCEL se puede implementar fácilmente utilizando LangServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f1163-0f20-49c5-ba03-aaa54b0dc20c",
   "metadata": {},
   "source": [
    "# Comenzar\n",
    "LCEL facilita la construcción de cadenas complejas a partir de componentes básicos y admite funcionalidades listas para usar, como transmisión, paralelismo y registro.\n",
    "\n",
    "## Ejemplo básico: prompt + modelo + analizador de salida\n",
    "El caso de uso más básico y común es encadenar una plantilla de prompt y un modelo. Para ver cómo funciona esto, creemos una cadena que tome un tema y genere un chiste:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfb6775-2324-4276-8cfd-95cea6eb0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-core langchain-community langchain-openai\n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19dedebc-9f6d-48a4-a690-778c05e812e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me a short joke about \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser()\n\u001b[1;32m      9\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m output_parser\n",
      "File \u001b[0;32m~/anaconda3/envs/LangChain/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/LangChain/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7ca94-2e27-4d4b-8d00-6a01ca8feca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
