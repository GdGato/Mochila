{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14427219-f02c-4399-ad13-fcb307c6e4e4",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "El Lenguaje de Expresión de Cadenas de LangChain, o LCEL, es una manera declarativa de componer fácilmente cadenas juntas. El LCEL fue diseñado desde el primer día para admitir la implementación de prototipos en producción, sin cambios de código, desde la cadena más simple de \"prompt + LLM\" hasta las cadenas más complejas (hemos visto a personas ejecutar con éxito cadenas LCEL con 100s de pasos en producción). Para resaltar algunas de las razones por las que podrías querer usar LCEL:\n",
    "\n",
    "Soporte de transmisión: al construir tus cadenas con LCEL, obtienes el mejor tiempo posible hasta el primer token (tiempo transcurrido hasta que sale el primer fragmento de salida). Para algunas cadenas, esto significa, por ejemplo, transmitir tokens directamente desde un LLM a un analizador de salida en continuo, y obtener fragmentos analizados e incrementales de salida a la misma velocidad que el proveedor de LLM emite los tokens en bruto.\n",
    "\n",
    "Soporte asíncrono: cualquier cadena construida con LCEL puede ser llamada tanto con la API síncrona (por ejemplo, en tu cuaderno Jupyter durante la prototipificación) como con la API asíncrona (por ejemplo, en un servidor LangServe). Esto permite utilizar el mismo código para prototipos y en producción, con un rendimiento excelente y la capacidad de manejar muchas solicitudes concurrentes en el mismo servidor.\n",
    "\n",
    "Ejecución paralela optimizada: siempre que las cadenas LCEL tengan pasos que se puedan ejecutar en paralelo (por ejemplo, si recuperas documentos de varios recuperadores), lo hacemos automáticamente, tanto en las interfaces síncronas como asíncronas, para la latencia más pequeña posible.\n",
    "\n",
    "Reintentos y alternativas: configura reintentos y alternativas para cualquier parte de tu cadena LCEL. Esta es una excelente manera de hacer que tus cadenas sean más confiables a escala. Actualmente estamos trabajando en agregar soporte de transmisión para reintentos/alternativas, para que puedas obtener confiabilidad adicional sin costo de latencia.\n",
    "\n",
    "Acceso a resultados intermedios: para cadenas más complejas, a menudo es muy útil acceder a los resultados de pasos intermedios incluso antes de que se produzca la salida final. Esto se puede usar para informar a los usuarios finales que algo está sucediendo, o incluso solo para depurar tu cadena. Puedes transmitir resultados intermedios, y está disponible en cada servidor LangServe.\n",
    "\n",
    "Esquemas de entrada y salida: los esquemas de entrada y salida proporcionan a cada cadena LCEL esquemas Pydantic y JSONSchema inferidos a partir de la estructura de tu cadena. Esto se puede usar para la validación de entradas y salidas, y es una parte integral de LangServe.\n",
    "\n",
    "Integración transparente con el seguimiento de LangSmith: a medida que tus cadenas se vuelven más y más complejas, es cada vez más importante entender qué está sucediendo exactamente en cada paso. Con LCEL, todos los pasos se registran automáticamente en LangSmith para una máxima observabilidad y capacidad de depuración.\n",
    "\n",
    "Integración transparente con el despliegue de LangServe: cualquier cadena creada con LCEL se puede implementar fácilmente utilizando LangServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f1163-0f20-49c5-ba03-aaa54b0dc20c",
   "metadata": {},
   "source": [
    "# Comenzar\n",
    "LCEL facilita la construcción de cadenas complejas a partir de componentes básicos y admite funcionalidades listas para usar, como transmisión, paralelismo y registro.\n",
    "\n",
    "## Ejemplo básico: prompt + modelo + analizador de salida\n",
    "El caso de uso más básico y común es encadenar una plantilla de prompt y un modelo. Para ver cómo funciona esto, creemos una cadena que tome un tema y genere un chiste:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfb6775-2324-4276-8cfd-95cea6eb0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install --upgrade --quiet langchain-core langchain-community langchain-openai\n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19dedebc-9f6d-48a4-a690-778c05e812e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Sabes cuál es el mejor momento para hacer una tarea que has estado posponiendo durante semanas? ¡Mañana!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Cuentame un chiste de {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"procrastinación\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc88d33-c123-4652-bca7-1370f42a73da",
   "metadata": {},
   "source": [
    "Observa esta línea de código, donde ensamblamos diferentes componentes en una única cadena utilizando LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79a74f8-f65f-400d-9c2d-ae631ce8f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b64817-3205-462e-92e4-86cac31c3eaa",
   "metadata": {},
   "source": [
    "El símbolo | es similar a un operador de tubería de Unix, que encadena los diferentes componentes alimentando la salida de un componente como entrada al siguiente componente.\n",
    "\n",
    "En esta cadena, la entrada del usuario se pasa a la plantilla de prompt, luego la salida de la plantilla de prompt se pasa al modelo, y finalmente, la salida del modelo se pasa al analizador de salida. Echemos un vistazo a cada componente individualmente para entender realmente lo que está sucediendo.\n",
    "\n",
    "## 1.  Prompt\n",
    "`prompt` es un `BasePromptTemplate`, lo que significa que toma un diccionario de variables de plantilla y produce un `PromptValue`. Un `PromptValue` es un contenedor alrededor de un prompt completado que se puede pasar tanto a un LLM (que toma una cadena como entrada) como a un ChatModel (que toma una secuencia de mensajes como entrada). Puede trabajar con ambos tipos de modelos de lenguaje porque define lógica tanto para producir `BaseMessages` como para producir una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36ab435-f519-44cc-8dbc-8ff258a721b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Cuentame un chiste de Nieve Napolitana')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"Nieve Napolitana\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eadf0437-3129-4fc7-92cf-62cd50cb5be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Cuentame un chiste de Nieve Napolitana')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b051c87-01dd-4c03-9e43-42d55e6515eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Cuentame un chiste de Nieve Napolitana'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fa3f7-8b61-4752-942a-a1e92bbb75ef",
   "metadata": {},
   "source": [
    "## 2.  Modelo \n",
    "El `PromptValue` se pasa al modelo. En este caso, nuestro modelo es un `ChatModel`, lo que significa que producirá un `BaseMessage` como salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c8ea47-72be-41a8-a893-1391b537e328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¿Cuál es el postre favorito de los esquimales? ¡La nieve napolitana!')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642f513-659e-4d0d-89f2-e12891a254d3",
   "metadata": {},
   "source": [
    "Si nuestro modelo fuera un LLM, generaría una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ebcf36-975a-415e-87ed-82c82011fa2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n¿Por qué la nieve napolitana siempre está tan fría?\\n\\nPorque siempre está en el congelador de la pizza. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eabd6a-5c77-4a83-baa0-a8302e37cd55",
   "metadata": {},
   "source": [
    "## 3.  Analizador de salida\n",
    "Y finalmente, pasamos la salida de nuestro modelo al `output_parser`, que es un `BaseOutputParser`, lo que significa que toma como entrada tanto una cadena como un `BaseMessage`. El `StrOutputParser`, en particular, convierte de manera simple cualquier entrada en una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0206861f-e09f-4822-8ed9-0f5f712f3a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Cuál es el postre favorito de los esquimales? ¡La nieve napolitana!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e9858-1b6d-4e10-a44a-8ca86cf7c62f",
   "metadata": {},
   "source": [
    "## 4.  Toda la tubería (Pipeline completa)\n",
    "\n",
    "Para seguir los pasos:\n",
    "\n",
    "1.  Ingresamos la entrada del usuario sobre el tema deseado, por ejemplo, {\"topic\": \"helado\"}.\n",
    "2.  El componente de prompt toma la entrada del usuario, que luego se utiliza para construir un `PromptValue` después de usar el tema para construir el prompt.\n",
    "3.  El componente del modelo toma el prompt generado y lo pasa al modelo de lenguaje (LLM) de OpenAI para su evaluación. La salida generada por el modelo es un objeto `ChatMessage`.\n",
    "4.  Finalmente, el componente de `output_parser` recibe un `ChatMessage` y lo transforma en una cadena de Python, que se devuelve desde el método de invocación.\n",
    "\n",
    "Estructura:\n",
    "-----------\n",
    "\n",
    "*   `Dict`: Entrada: `topic=helado`\n",
    "*   `PromptTemplate`: Componente de prompt\n",
    "*   `ChatModel`: Componente de modelo\n",
    "*   `StrOutputParser`: Componente de análisis de salida\n",
    "*   Resultado: Cadena de salida\n",
    "\n",
    "Ten en cuenta que si tienes curiosidad acerca de la salida de cualquier componente, siempre puedes probar una versión más pequeña de la cadena, como `prompt` o `prompt | model`, para ver los resultados intermedios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f291739-322e-4aca-acfb-6ff8f10d23b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¿Cuál es el helado más peligroso? El helado asesino, ¡porque mata de frío!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"topic\": \"helado\"}\n",
    "\n",
    "prompt.invoke(input)\n",
    "# > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n",
    "\n",
    "(prompt | model).invoke(input)\n",
    "# > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cdcb0-f17b-4943-8401-540ea810c1c9",
   "metadata": {},
   "source": [
    "# Ejemplo de Búsqueda RAG\n",
    "\n",
    "Para nuestro próximo ejemplo, queremos ejecutar una cadena de generación aumentada con recuperación (RAG) para agregar contexto al responder preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3213e3d0-0f06-404b-945b-ad78382a2128",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (0.1.6)\n",
      "Requirement already satisfied: docarray in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (0.40.0)\n",
      "Requirement already satisfied: tiktoken in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (0.0.19)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (0.1.22)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from docarray) (3.9.13)\n",
      "Requirement already satisfied: rich>=13.1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from docarray) (13.7.0)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from docarray) (2.31.0.20240125)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain docarray tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe65424-70a9-4e1f-8732-4c7ef3e15fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison trabajaba en Kensho.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"¿Dónde trabajaba Harrison?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5acabf-483c-4fb3-85b8-f14792e74e7f",
   "metadata": {},
   "source": [
    "En este caso, la cadena compuesta es:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c2815e-c2a9-441c-80bf-f7e8cf31e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec112a9-cebf-4a2a-9641-598eef9f16ba",
   "metadata": {},
   "source": [
    "Para explicar esto, primero podemos observar que la plantilla de prompt anterior toma el contexto y la pregunta como valores a ser sustituidos en el prompt. Antes de construir la plantilla de prompt, queremos recuperar documentos relevantes para la búsqueda e incluirlos como parte del contexto.\n",
    "\n",
    "Como paso preliminar, hemos configurado el recuperador utilizando un almacenamiento en memoria, que puede recuperar documentos basados en una consulta. Este también es un componente ejecutable que se puede encadenar con otros componentes, pero también puedes intentar ejecutarlo por separado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "884d574d-962e-4278-afc3-ffaca2a34603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho'),\n",
       " Document(page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"¿Dónde trabajaba Harrison?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5ddb4-8533-40c2-b10d-113492b97511",
   "metadata": {},
   "source": [
    "Luego, utilizamos el componente `RunnableParallel` para preparar las entradas esperadas en el prompt, utilizando las entradas para los documentos recuperados, así como la pregunta original del usuario. Utilizamos el recuperador para la búsqueda de documentos y `RunnablePassthrough` para pasar la pregunta del usuario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a74ef16b-7d6e-44fa-a666-93e597175185",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4897cbc-6c41-4a8d-86dd-83c8e2ea28c8",
   "metadata": {},
   "source": [
    "Para repasar, la cadena completa es:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f2b650-9009-4800-bc74-99139e1b0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24939148-b23a-4ca5-9590-a3cb460a6df1",
   "metadata": {},
   "source": [
    "Con el flujo siendo:\n",
    "\n",
    "1. Los primeros pasos crean un objeto RunnableParallel con dos entradas. La primera entrada, context, incluirá los resultados de los documentos recuperados por el recuperador. La segunda entrada, question, contendrá la pregunta original del usuario. Para pasar la pregunta, utilizamos RunnablePassthrough para copiar esta entrada.\n",
    "\n",
    "2. Alimentamos el diccionario del paso anterior al componente de prompt. Luego, toma la entrada del usuario, que es la pregunta, así como el documento recuperado, que es el contexto, para construir un prompt y producir un PromptValue.\n",
    "\n",
    "3. El componente del modelo toma el prompt generado y lo pasa al modelo de lenguaje (LLM) de OpenAI para su evaluación. La salida generada por el modelo es un objeto ChatMessage.\n",
    "\n",
    "4. Finalmente, el componente de output_parser recibe un ChatMessage y lo transforma en una cadena de Python, que se devuelve desde el método de invocación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a357c98-1f68-4b65-a1eb-b81c5ae31d10",
   "metadata": {},
   "source": [
    "# Pasos siguientes\n",
    "Recomendamos leer nuestra sección [\"Why use LCEL\"](https://python.langchain.com/docs/expression_language/why) a continuación para ver una comparación lado a lado del código necesario para producir funcionalidades comunes con y sin LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93084447-5587-4563-9d3e-ce8a970570a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
