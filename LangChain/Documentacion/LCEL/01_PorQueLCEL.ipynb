{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Por qué usar LCEL\n",
    "Recomendamos leer primero la sección de Inicio rápido de LCEL.\n",
    "\n",
    "LCEL facilita la construcción de cadenas complejas a partir de componentes básicos. Lo hace proporcionando:\n",
    "\n",
    "1. **Una interfaz unificada:** Cada objeto LCEL implementa la interfaz Runnable, que define un conjunto común de métodos de invocación (invoke, batch, stream, ainvoke, ...). Esto hace posible que las cadenas de objetos LCEL también admitan automáticamente estas invocaciones. Es decir, cada cadena de objetos LCEL es en sí misma un objeto LCEL.\n",
    "\n",
    "2. **Primitivas de composición:** LCEL proporciona varias primitivas que facilitan la composición de cadenas, la paralelización de componentes, la adición de alternativas, la configuración dinámica interna de la cadena y más.\n",
    "\n",
    "Para entender mejor el valor de LCEL, es útil verlo en acción y pensar en cómo podríamos recrear funcionalidades similares sin él. En este recorrido, haremos precisamente eso con nuestro ejemplo básico de la sección de inicio rápido. Tomaremos nuestra cadena simple de prompt + modelo, que en el fondo ya define mucha funcionalidad, y veremos qué sería necesario hacer para recrear todo eso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración\n",
    "## Dependencias\n",
    "Utilizaremos un modelo de chat de OpenAI y embeddings, y un vector store Chroma en este recorrido, pero todo lo mostrado aquí funciona con cualquier ChatModel o LLM, Embeddings, y VectorStore o Retriever.\n",
    "\n",
    "Utilizaremos los siguientes paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos establecer la variable de entorno OPENAI_API_KEY, lo cual se puede hacer directamente o cargarse desde un archivo .env de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import getpass\n",
    "#import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí está la aplicación de preguntas y respuestas que construimos sobre la publicación del blog \"Agentes Autónomos Potenciados por LLM\" de Lilian Weng en el Inicio Rápido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar, dividir e indexar el contenido del blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Recuperar y generar utilizando los fragmentos relevantes del blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La descomposición de tareas es un proceso en el que se divide un problema o tarea complicada en pasos más pequeños y manejables. Esto permite que un agente o modelo de IA pueda planificar y abordar la tarea de manera más efectiva, resolviendo cada paso por separado. Se pueden utilizar diferentes enfoques para la descomposición de tareas, como la generación de árboles de pensamiento o instrucciones específicas para cada tarea.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"¿Qué es la descomposición de tareas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextualizando la pregunta\n",
    "Primero, necesitaremos definir una subcadena que tome mensajes históricos y la última pregunta del usuario, y reformule la pregunta si hace referencia a alguna información en la información histórica.\n",
    "\n",
    "Utilizaremos un indicador que incluya una variable MessagesPlaceholder bajo el nombre \"chat_history\". Esto nos permite pasar una lista de mensajes al indicador utilizando la clave de entrada \"chat_history\", y estos mensajes se insertarán después del mensaje del sistema y antes del mensaje humano que contiene la última pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando esta cadena podemos hacer preguntas de seguimiento que hagan referencia a mensajes pasados ​​y reformularlos en preguntas independientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Cuál es la definición de \"grande\"?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "    \"chat_history\": [\n",
    "    HumanMessage(content=\"¿Qué significa LLM?\"),\n",
    "    AIMessage(content=\"Modelo de lenguaje grande\"),\n",
    "],\n",
    "    \"question\": \"¿Qué se entiende por grande\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Y ahora podemos construir nuestra cadena completa de preguntas y respuestas (QA).\n",
    "\n",
    "Observa que agregamos funcionalidad de enrutamiento para ejecutar solo la \"cadena de pregunta condensada\" cuando nuestro historial de chat no está vacío. Aquí aprovechamos el hecho de que si una función en una cadena LCEL devuelve otra cadena, esa cadena también se invocará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "Translate to spanish\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hay varias formas comunes de realizar la descomposición de tareas:\\n\\n1. Mediante el uso de técnicas de prompting simples, como el Chain of Thought (CoT), donde se instruye al modelo a pensar paso a paso y descomponer la tarea en subobjetivos más pequeños.\\n\\n2. Utilizando instrucciones específicas de la tarea, como \"Escribe un esquema de la historia\" para escribir una novela, que ayudan al modelo a entender los pasos necesarios para completar la tarea.\\n\\n3. Con la ayuda de aportes humanos, donde los humanos proporcionan información adicional o guían al modelo en la descomposición de la tarea. Esto puede ser útil cuando la tarea es compleja o requiere conocimientos específicos que el modelo no posee.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echa un vistazo a la traza de LangSmith\n",
    "\n",
    "Aquí hemos explicado cómo agregar lógica de aplicación para incorporar salidas históricas, pero aún actualizamos manualmente el historial del chat e lo insertamos en cada entrada. En una aplicación de preguntas y respuestas real, desearemos alguna manera de persistir el historial del chat y alguna manera de insertarlo y actualizarlo automáticamente.\n",
    "\n",
    "Para esto podemos usar:\n",
    "\n",
    "BaseChatMessageHistory: Almacena el historial del chat. RunnableWithMessageHistory: Envoltura para una cadena LCEL y un BaseChatMessageHistory que se encarga de inyectar el historial del chat en las entradas y actualizarlo después de cada invocación. Para obtener una guía detallada sobre cómo usar estas clases juntas para crear una cadena de conversación con estado, dirígete a la página de Cómo agregar historial de mensajes (memoria) de LCEL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
