{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de562de5-c900-45b5-98d7-13137a719cc1",
   "metadata": {},
   "source": [
    "# RAG ü¶úÔ∏èüîó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9e2bd-78ff-4bef-b7b1-3270a75e7732",
   "metadata": {},
   "source": [
    "Veamos c√≥mo agregar un paso de recuperaci√≥n a un prompt y LLM, lo que suma a una cadena de \"generaci√≥n aumentada con recuperaci√≥n\" (retrieval-augmented generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307cab1c-1e20-45fe-a734-0e7a29474741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b6fbe4-79f6-4535-9d79-b8fc9aad8f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e897d771-7b2b-47ef-8a94-143ac84846e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e3f693-779f-4d3d-a2d4-9f67836516d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parte 01\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison trabaj√≥ en kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Responda la pregunta bas√°ndose √∫nicamente en el siguiente contexto:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cadba1-7a91-4107-a08d-6665b02ae776",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Este c√≥digo est√° creando una base de datos de vectores utilizando la biblioteca FAISS y OpenAI Embeddings, y luego utilizando esa base de datos para crear un sistema de pregunta-respuesta. Aqu√≠ hay un desglose del c√≥digo:\n",
    "\n",
    "`vectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())`\n",
    "\n",
    "Esta l√≠nea crea una base de datos de vectores utilizando la biblioteca FAISS e incrusta el texto \"harrison worked at kensho\" utilizando el modelo de OpenAI Embeddings. La base de datos de vectores resultante se almacena en la variable `vectorstore`.\n",
    "\n",
    "`retriever = vectorstore.as_retriever()`\n",
    "\n",
    "Esta l√≠nea crea un objeto recuperador que se puede utilizar para buscar en la base de datos de vectores y recuperar vectores relevantes seg√∫n una consulta dada.\n",
    "\n",
    "`template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\"`\n",
    "\n",
    "Esta l√≠nea define una cadena de plantilla que se utilizar√° para generar prompts para el sistema de pregunta-respuesta. El marcador de posici√≥n {context} se reemplazar√° con el contexto real (es decir, el texto incrustado) y el marcador de posici√≥n {question} se reemplazar√° con la pregunta real.\n",
    "\n",
    "`prompt = ChatPromptTemplate.from_template(template)`\n",
    "\n",
    "Esta l√≠nea crea un objeto `ChatPromptTemplate` utilizando la cadena de plantilla definida anteriormente.\n",
    "\n",
    "`model = ChatOpenAI()`\n",
    "\n",
    "Esta l√≠nea crea un modelo de pregunta-respuesta utilizando la clase `ChatOpenAI`. Este modelo se puede utilizar para generar respuestas a preguntas basadas en el contexto proporcionado.\n",
    "\n",
    "Para modificar este c√≥digo, puedes cambiar el texto incrustado en la base de datos de vectores modificando la lista de cadenas que se pasa al m√©todo `FAISS.from_texts()`. Tambi√©n puedes modificar la plantilla de prompt cambiando la cadena asignada a la variable de plantilla. Finalmente, puedes utilizar un modelo de pregunta-respuesta diferente reemplazando la llamada `ChatOpenAI()` con una instancia de una clase de modelo diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf24b323-a5d7-487e-a598-7fa93f81da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "248026b5-cc8f-4b30-bbd1-f365a514477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison trabajaba en Kensho.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"¬øD√≥nde trabajaba Harrison?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d4e814-04de-489b-9932-041ed29fb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "194b84c6-8dda-4bda-ba35-98003b50212e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison trabaj√≥ en Kensho.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"espa√±ol\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873e213-bbae-4392-a8a8-1b7eb87b9e61",
   "metadata": {},
   "source": [
    "# Cadena de recuperaci√≥n conversacional\n",
    "\n",
    "Podemos a√±adir f√°cilmente historial de conversaci√≥n. Esto significa principalmente agregar `chat_message_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e898d31-e39e-4aee-947f-08e94016153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488a574a-29f6-4f57-ae2b-37adf5ef0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f3ba3b3-25b5-4229-a888-89e3b8d76804",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4917b699-6d49-44db-8bcb-d30ead601c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff0a962-8b70-40e2-abd4-59ee7598f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16bf5b56-981d-4307-a3ff-8750b401ed0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Harrison was employed at Kensho.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did harrison work?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862b836f-ee64-4547-87a0-21b64fbfb9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Harrison worked at Kensho.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did he work?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"Who wrote this notebook?\"),\n",
    "            AIMessage(content=\"Harrison\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e307bb7-b539-4e02-a469-6cbf80120d25",
   "metadata": {},
   "source": [
    "# Con memoria y devoluci√≥n de documentos fuente \n",
    "Esto muestra c√≥mo usar la memoria con lo anterior. Para la memoria, necesitamos gestionarla fuera, en la memoria. Para devolver los documentos recuperados, solo necesitamos pasarlos todo el camino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88ac43fd-25ac-4d37-8b43-854d14c31bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f30586dd-b502-4332-acd6-101ab938c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "438af37a-2422-43fb-9b53-d224bc10c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f45566d-d067-4e8f-a57d-c20346ad9c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Harrison was employed at Kensho.'),\n",
       " 'docs': [Document(page_content='harrison trabaj√≥ en kensho')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"where did harrison work?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c10e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fc6499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='where did harrison work?'),\n",
       "  AIMessage(content='Harrison was employed at Kensho.')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "714c74bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Harrison actually worked at Kensho.'),\n",
       " 'docs': [Document(page_content='harrison trabaj√≥ en kensho')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"but where did he really work?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab0d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
