{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio Rapido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain tiene varios componentes diseñados para ayudar a construir aplicaciones de preguntas y respuestas, y aplicaciones RAG de manera más general. Para familiarizarnos con ellos, construiremos una aplicación simple de preguntas y respuestas sobre una fuente de datos de texto. En el proceso, revisaremos una arquitectura típica de preguntas y respuestas, discutiremos los componentes relevantes de LangChain y resaltaremos recursos adicionales para técnicas más avanzadas de preguntas y respuestas. También veremos cómo LangSmith puede ayudarnos a rastrear y entender nuestra aplicación. LangSmith será cada vez más útil a medida que nuestra aplicación crezca en complejidad.\n",
    "\n",
    "Arquitectura\n",
    "Crearemos una aplicación RAG típica según se describe en la introducción de preguntas y respuestas, que consta de dos componentes principales:\n",
    "\n",
    "1. Indexación: una canalización para ingerir datos desde una fuente e indexarlos. Esto suele ocurrir offline.\n",
    "\n",
    "2. Recuperación y generación: la cadena RAG real, que toma la consulta del usuario en tiempo de ejecución y recupera los datos relevantes del índice, luego los pasa al modelo.\n",
    "\n",
    "La secuencia completa, desde datos en bruto hasta la respuesta, se verá así:\n",
    "\n",
    "**Indexación**\n",
    "- Carga: Primero necesitamos cargar nuestros datos. Utilizaremos DocumentLoaders para esto.\n",
    "- División: Los separadores de texto dividen documentos grandes en fragmentos más pequeños. Esto es útil tanto para indexar datos como para pasarlos a un modelo, ya que los fragmentos grandes son más difíciles de buscar y no caben en la ventana de contexto finito de un modelo.\n",
    "- Almacenamiento: Necesitamos un lugar para almacenar e indexar nuestros fragmentos, para que puedan buscarse posteriormente. Esto se hace a menudo utilizando un VectorStore y un modelo de embeddings.\n",
    "\n",
    "**Recuperación y generación**\n",
    "- Recuperación: Dada una entrada del usuario, se recuperan los fragmentos relevantes del almacenamiento utilizando un Recuperador.\n",
    "- Generación: Un ChatModel / LLM produce una respuesta utilizando un prompt que incluye la pregunta y los datos recuperados.\n",
    "\n",
    "Configuración\n",
    "Dependencias\n",
    "Utilizaremos un modelo de chat de OpenAI, embeddings y un vector store Chroma en este tutorial, pero todo lo mostrado aquí funciona con cualquier modelo de chat o LLM, embeddings, y vector store o recuperador.\n",
    "\n",
    "Utilizaremos los siguientes paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos configurar la variable de entorno OPENAI_API_KEY, que se puede hacer directamente o cargar desde un archivo .env de esta manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith\n",
    "\n",
    "Muchas de las aplicaciones que construyas con LangChain contendrán múltiples pasos con múltiples invocaciones de llamadas a LLM. A medida que estas aplicaciones se vuelven más complejas, se vuelve crucial poder inspeccionar qué está sucediendo exactamente dentro de tu cadena o agente. La mejor manera de hacer esto es con LangSmith.\n",
    "\n",
    "Cabe destacar que LangSmith no es necesario, pero es útil. Si deseas utilizar LangSmith, después de registrarte en el enlace anterior, asegúrate de configurar tus variables de entorno para comenzar a registrar trazas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vista previa\n",
    "\n",
    "En esta guía, construiremos una aplicación de preguntas y respuestas sobre la publicación del blog \"LLM Powered Autonomous Agents\" de Lilian Weng, lo que nos permitirá hacer preguntas sobre el contenido de la publicación.\n",
    "\n",
    "Podemos crear un simple flujo de trabajo de indexación y una cadena RAG para lograr esto en ~20 líneas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La descomposición de tareas es un proceso en el que se divide un problema o tarea complicada en pasos más pequeños y manejables. Esto permite que un agente o modelo de IA pueda abordar la tarea de manera más efectiva al enfocarse en cada paso por separado. Se pueden utilizar diferentes técnicas, como el uso de instrucciones específicas o la generación de múltiples pensamientos por paso, para llevar a cabo la descomposición de tareas.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"¿Que es la descomposición de tareas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paseo detallado\n",
    "\n",
    "Vamos a revisar el código anterior paso a paso para comprender realmente lo que está sucediendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indexación: Carga\n",
    "Primero, necesitamos cargar el contenido del blog. Podemos utilizar DocumentLoaders para esto, que son objetos que cargan datos desde una fuente y devuelven una lista de Documentos. Un Documento es un objeto con algún page_content (str) y metadata (dict).\n",
    "\n",
    "En este caso, utilizaremos WebBaseLoader, que utiliza urllib para cargar HTML desde URL web y BeautifulSoup para analizarlo a texto. Podemos personalizar la conversión de HTML a texto pasando parámetros al analizador BeautifulSoup a través de bs_kwargs (consultar la documentación de BeautifulSoup). En este caso, solo son relevantes las etiquetas HTML con las clases \"post-content\", \"post-title\" o \"post-header\", por lo que eliminaremos todas las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "**DocumentLoader:** Objeto que carga datos desde una fuente como una lista de Documentos.\n",
    "- **Documentación:** Documentación detallada sobre cómo utilizar DocumentLoaders.\n",
    "- **Integraciones:** Más de 160 integraciones para elegir.\n",
    "- **Interfaz:** Referencia de API para la interfaz base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexación: División\n",
    "\n",
    "Nuestro documento cargado tiene más de 42,000 caracteres de longitud. Esto es demasiado largo para ajustarse en la ventana de contexto de muchos modelos. Incluso para aquellos modelos que podrían ajustar la publicación completa en su ventana de contexto, los modelos pueden tener dificultades para encontrar información en entradas muy largas.\n",
    "\n",
    "Para manejar esto, dividiremos el Documento en fragmentos para la inserción de vectores. Esto debería ayudarnos a recuperar solo las partes más relevantes del blog en tiempo de ejecución.\n",
    "\n",
    "En este caso, dividiremos nuestros documentos en fragmentos de 1000 caracteres con 200 caracteres de superposición entre fragmentos. La superposición ayuda a mitigar la posibilidad de separar una declaración de un contexto importante relacionado con ella. Utilizamos RecursiveCharacterTextSplitter, que dividirá recursivamente el documento utilizando separadores comunes como nuevas líneas hasta que cada fragmento tenga el tamaño adecuado. Este es el separador de texto recomendado para casos de uso de texto genérico.\n",
    "\n",
    "Configuramos add_start_index=True para que el índice de caracteres en el que comienza cada Documento dividido dentro del Documento inicial se conserve como atributo de metadatos \"start_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "**TextSplitter:** Objeto que divide una lista de Documentos en fragmentos más pequeños. Subclase de DocumentTransformers.\n",
    "- **Explora Separadores conscientes del contexto, que mantienen la ubicación (\"contexto\") de cada fragmento en el Documento original:**\n",
    "  - [Archivos Markdown](https://python.langchain.com/docs/modules/data_connection/document_transformers/markdown_header_metadata)\n",
    "\n",
    "  - [Código (py o js)](https://python.langchain.com/docs/integrations/document_loaders/source_code)\n",
    "  - [Documentos científicos](https://python.langchain.com/docs/integrations/document_loaders/grobid)\n",
    "- **[Interfaz](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.TextSplitter.html):** Referencia de API para la interfaz base.\n",
    "\n",
    "**DocumentTransformer:** Objeto que realiza una transformación en una lista de Documentos.\n",
    "- **Documentación:** Documentación detallada sobre cómo utilizar DocumentTransformers.\n",
    "- **Integraciones:** Integraciones disponibles.\n",
    "- **Interfaz:** Referencia de API para la interfaz base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Indexación: Almacenamiento\n",
    "\n",
    "Ahora necesitamos indexar nuestros 66 fragmentos de texto para que podamos buscar en ellos en tiempo de ejecución. La forma más común de hacer esto es incrustar el contenido de cada fragmento de documento e insertar estas incrustaciones en una base de datos de vectores (o vector store). Cuando queremos buscar en nuestros fragmentos, tomamos una consulta de búsqueda de texto, la incrustamos y realizamos algún tipo de búsqueda de \"similitud\" para identificar los fragmentos almacenados con las incrustaciones más similares a nuestra incrustación de consulta. La medida de similitud más simple es la similitud coseno; medimos el coseno del ángulo entre cada par de incrustaciones (que son vectores de alta dimensión).\n",
    "\n",
    "Podemos incrustar y almacenar todos nuestros fragmentos de documentos en un solo comando utilizando el vector store Chroma y el modelo [OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "**Embeddings:** Envoltura alrededor de un modelo de incrustación de texto, utilizado para convertir texto en incrustaciones.\n",
    "- **Documentación:** Documentación detallada sobre cómo utilizar incrustaciones.\n",
    "- **Integraciones:** Más de 30 integraciones para elegir.\n",
    "- **Interfaz:** Referencia de API para la interfaz base.\n",
    "\n",
    "**VectorStore:** Envoltura alrededor de una base de datos de vectores, utilizado para almacenar y consultar incrustaciones.\n",
    "- **Documentación:** Documentación detallada sobre cómo utilizar vector stores.\n",
    "- **Integraciones:** Más de 40 integraciones para elegir.\n",
    "- **Interfaz:** Referencia de API para la interfaz base.\n",
    "\n",
    "Esto completa la parte de Indexación del flujo de trabajo. En este punto, tenemos un vector store consultable que contiene los contenidos fragmentados de nuestra publicación de blog. Dada una pregunta del usuario, idealmente deberíamos poder devolver los fragmentos de la publicación de blog que responden a la pregunta.\n",
    "\n",
    "# 4. Recuperación y Generación: Recuperar\n",
    "\n",
    "Ahora escribamos la lógica real de la aplicación. Queremos crear una aplicación sencilla que tome una pregunta del usuario, busque documentos relevantes para esa pregunta, pase los documentos recuperados y la pregunta inicial a un modelo y devuelva una respuesta.\n",
    "\n",
    "Primero, necesitamos definir nuestra lógica para buscar documentos. LangChain define una interfaz de Retriever que envuelve un índice que puede devolver Documentos relevantes dada una consulta de cadena.\n",
    "\n",
    "El tipo más común de Retriever es VectorStoreRetriever, que utiliza las capacidades de búsqueda de similitud de un vector store para facilitar la recuperación. Cualquier VectorStore se puede convertir fácilmente en un Retriever con VectorStore.as_retriever():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"¿Cuáles son los enfoques para la descomposición de tareas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "Los vector stores se utilizan comúnmente para la recuperación, pero también hay otras formas de hacer recuperación.\n",
    "\n",
    "**Retriever:** Un objeto que devuelve Documentos dado una consulta de texto.\n",
    "\n",
    "- **Documentación:** Documentación adicional sobre la interfaz y las técnicas de recuperación incorporadas. Algunas de ellas incluyen:\n",
    "  - MultiQueryRetriever genera variantes de la pregunta de entrada para mejorar la tasa de éxito de la recuperación.\n",
    "  - MultiVectorRetriever (diagrama a continuación) en cambio genera variantes de las incrustaciones, también para mejorar la tasa de éxito de la recuperación.\n",
    "  - Max marginal relevance selecciona relevancia y diversidad entre los documentos recuperados para evitar pasar contexto duplicado.\n",
    "  - Los documentos pueden filtrarse durante la recuperación del vector store mediante filtros de metadatos, como con un Self Query Retriever.\n",
    "\n",
    "- **Integraciones:** Integraciones con servicios de recuperación.\n",
    "  \n",
    "- **Interfaz:** Referencia de API para la interfaz base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recuperación y Generación: Generar\n",
    "\n",
    "Vamos a ponerlo todo junto en una cadena que toma una pregunta, recupera documentos relevantes, construye un indicador, lo pasa a un modelo y analiza la salida.\n",
    "\n",
    "Utilizaremos el modelo de chat de OpenAI gpt-3.5-turbo, pero se podría sustituir por cualquier LangChain LLM o ChatModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un mensaje para RAG que se registra en el centro de mensajes de LangChain (aquí)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos el protocolo LCEL Runnable para definir la cadena, lo que nos permitirá:\n",
    "- conectar componentes y funciones de manera transparente,\n",
    "- rastrear automáticamente nuestra cadena en LangSmith,\n",
    "- obtener llamadas en streaming, asíncronas y por lotes directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La descomposición de tareas es el proceso de dividir una tarea compleja en pasos más pequeños y manejables. Esto permite abordar la tarea de manera más eficiente y facilita la planificación y ejecución de cada paso. La descomposición de tareas puede realizarse mediante instrucciones específicas, como en el caso de escribir un esquema de una historia, o mediante la interacción con un modelo de IA."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"¿Qué es la descomposición de tareas?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulte el rastro de [LangSmith]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "**Choosing a model:**\n",
    "\n",
    "- **ChatModel:** Un modelo de chat respaldado por LLM. Toma una secuencia de mensajes y devuelve un mensaje.\n",
    "  - **Documentación:** Documentación detallada.\n",
    "  - **Integraciones:** Más de 25 integraciones para elegir.\n",
    "  - **Interfaz:** Referencia de API para la interfaz base.\n",
    "\n",
    "- **LLM:** Un modelo de lenguaje que toma un texto de entrada y devuelve un texto de salida.\n",
    "  - **Documentación:** Documentación detallada.\n",
    "  - **Integraciones:** Más de 75 integraciones para elegir.\n",
    "  - **Interfaz:** Referencia de API para la interfaz base.\n",
    "\n",
    "Vea una guía sobre RAG con modelos que se ejecutan localmente aquí.\n",
    "\n",
    "**Personalización del indicador:**\n",
    "\n",
    "Como se muestra anteriormente, podemos cargar indicadores (por ejemplo, este indicador RAG) desde el centro de indicadores. El indicador también se puede personalizar fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La descomposición de tareas es un proceso en el que se divide una tarea compleja en pasos más pequeños y manejables. Esto permite a un agente o modelo abordar la tarea de manera más efectiva al enfocarse en cada paso individualmente. Gracias por preguntar!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"¿Que es la descomposicion de tareas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próximos pasos\n",
    "\n",
    "Hemos cubierto mucho contenido en poco tiempo. Hay muchas funciones, integraciones y extensiones para explorar en cada una de las secciones anteriores. Además de las fuentes para profundizar mencionadas anteriormente, buenos próximos pasos incluyen:\n",
    "\n",
    "- **Fuentes de retorno:** Aprender a devolver documentos fuente.\n",
    "- **Streaming:** Aprender a transmitir salidas y pasos intermedios.\n",
    "- **Agregar historial de chat:** Aprender a agregar historial de chat a tu aplicación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
