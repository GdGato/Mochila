{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregar historial de chat\n",
    "En muchas aplicaciones de preguntas y respuestas, queremos permitir que el usuario tenga una conversación de ida y vuelta, lo que significa que la aplicación necesita alguna forma de \"memoria\" de preguntas y respuestas anteriores, y alguna lógica para incorporarlas en su pensamiento actual.\n",
    "\n",
    "En esta guía, nos enfocamos en agregar lógica para incorporar mensajes históricos y NO en la gestión del historial de chat. La gestión del historial de chat se cubre aquí.\n",
    "\n",
    "Trabajaremos con la aplicación de preguntas y respuestas que construimos sobre la publicación del blog \"Agentes Autónomos Potenciados por LLM\" de Lilian Weng en el Inicio Rápido. Necesitaremos actualizar dos cosas sobre nuestra aplicación existente:\n",
    "\n",
    "1. Indicador: Actualizar nuestro indicador para admitir mensajes históricos como entrada.\n",
    "2. Contextualización de preguntas: Agregar una subcadena que tome la última pregunta del usuario y la reformule en el contexto del historial de chat. Esto es necesario en caso de que la última pregunta haga referencia a algún contexto de mensajes anteriores. Por ejemplo, si un usuario hace una pregunta de seguimiento como \"¿Puedes ampliar sobre el segundo punto?\", esto no se puede entender sin el contexto del mensaje anterior. Por lo tanto, no podemos realizar eficazmente la recuperación con una pregunta como esta.\n",
    "\n",
    "# Configuración\n",
    "## Dependencias\n",
    "Utilizaremos un modelo de chat de OpenAI y embeddings, y un vector store Chroma en este recorrido, pero todo lo mostrado aquí funciona con cualquier ChatModel o LLM, Embeddings y VectorStore o Retriever.\n",
    "\n",
    "Usaremos los siguientes paquetes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
