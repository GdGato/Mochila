{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio Rapido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain tiene varios componentes diseñados para ayudar a construir aplicaciones de preguntas y respuestas, y aplicaciones RAG de manera más general. Para familiarizarnos con ellos, construiremos una aplicación simple de preguntas y respuestas sobre una fuente de datos de texto. En el proceso, revisaremos una arquitectura típica de preguntas y respuestas, discutiremos los componentes relevantes de LangChain y resaltaremos recursos adicionales para técnicas más avanzadas de preguntas y respuestas. También veremos cómo LangSmith puede ayudarnos a rastrear y entender nuestra aplicación. LangSmith será cada vez más útil a medida que nuestra aplicación crezca en complejidad.\n",
    "\n",
    "Arquitectura\n",
    "Crearemos una aplicación RAG típica según se describe en la introducción de preguntas y respuestas, que consta de dos componentes principales:\n",
    "\n",
    "1. Indexación: una canalización para ingerir datos desde una fuente e indexarlos. Esto suele ocurrir offline.\n",
    "\n",
    "2. Recuperación y generación: la cadena RAG real, que toma la consulta del usuario en tiempo de ejecución y recupera los datos relevantes del índice, luego los pasa al modelo.\n",
    "\n",
    "La secuencia completa, desde datos en bruto hasta la respuesta, se verá así:\n",
    "\n",
    "**Indexación**\n",
    "- Carga: Primero necesitamos cargar nuestros datos. Utilizaremos DocumentLoaders para esto.\n",
    "- División: Los separadores de texto dividen documentos grandes en fragmentos más pequeños. Esto es útil tanto para indexar datos como para pasarlos a un modelo, ya que los fragmentos grandes son más difíciles de buscar y no caben en la ventana de contexto finito de un modelo.\n",
    "- Almacenamiento: Necesitamos un lugar para almacenar e indexar nuestros fragmentos, para que puedan buscarse posteriormente. Esto se hace a menudo utilizando un VectorStore y un modelo de embeddings.\n",
    "\n",
    "**Recuperación y generación**\n",
    "- Recuperación: Dada una entrada del usuario, se recuperan los fragmentos relevantes del almacenamiento utilizando un Recuperador.\n",
    "- Generación: Un ChatModel / LLM produce una respuesta utilizando un prompt que incluye la pregunta y los datos recuperados.\n",
    "\n",
    "Configuración\n",
    "Dependencias\n",
    "Utilizaremos un modelo de chat de OpenAI, embeddings y un vector store Chroma en este tutorial, pero todo lo mostrado aquí funciona con cualquier modelo de chat o LLM, embeddings, y vector store o recuperador.\n",
    "\n",
    "Utilizaremos los siguientes paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos configurar la variable de entorno OPENAI_API_KEY, que se puede hacer directamente o cargar desde un archivo .env de esta manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith\n",
    "\n",
    "Muchas de las aplicaciones que construyas con LangChain contendrán múltiples pasos con múltiples invocaciones de llamadas a LLM. A medida que estas aplicaciones se vuelven más complejas, se vuelve crucial poder inspeccionar qué está sucediendo exactamente dentro de tu cadena o agente. La mejor manera de hacer esto es con LangSmith.\n",
    "\n",
    "Cabe destacar que LangSmith no es necesario, pero es útil. Si deseas utilizar LangSmith, después de registrarte en el enlace anterior, asegúrate de configurar tus variables de entorno para comenzar a registrar trazas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vista previa\n",
    "\n",
    "En esta guía, construiremos una aplicación de preguntas y respuestas sobre la publicación del blog \"LLM Powered Autonomous Agents\" de Lilian Weng, lo que nos permitirá hacer preguntas sobre el contenido de la publicación.\n",
    "\n",
    "Podemos crear un simple flujo de trabajo de indexación y una cadena RAG para lograr esto en ~20 líneas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La descomposición de tareas es un proceso en el que se divide un problema o tarea complicada en pasos más pequeños y manejables. Esto permite que un agente o modelo de IA pueda abordar la tarea de manera más efectiva al pensar paso a paso y utilizar la computación en tiempo de prueba para descomponer la tarea en sub-tareas más simples. La descomposición de tareas puede realizarse mediante instrucciones específicas, como prompts simples o instrucciones específicas para cada tarea, o con la ayuda de aportes humanos.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"¿Que es la descomposición de tareas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paseo detallado\n",
    "\n",
    "Vamos a revisar el código anterior paso a paso para comprender realmente lo que está sucediendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indexación: Carga\n",
    "Primero, necesitamos cargar el contenido del blog. Podemos utilizar DocumentLoaders para esto, que son objetos que cargan datos desde una fuente y devuelven una lista de Documentos. Un Documento es un objeto con algún page_content (str) y metadata (dict).\n",
    "\n",
    "En este caso, utilizaremos WebBaseLoader, que utiliza urllib para cargar HTML desde URL web y BeautifulSoup para analizarlo a texto. Podemos personalizar la conversión de HTML a texto pasando parámetros al analizador BeautifulSoup a través de bs_kwargs (consultar la documentación de BeautifulSoup). En este caso, solo son relevantes las etiquetas HTML con las clases \"post-content\", \"post-title\" o \"post-header\", por lo que eliminaremos todas las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "**DocumentLoader:** Objeto que carga datos desde una fuente como una lista de Documentos.\n",
    "- **Documentación:** Documentación detallada sobre cómo utilizar DocumentLoaders.\n",
    "- **Integraciones:** Más de 160 integraciones para elegir.\n",
    "- **Interfaz:** Referencia de API para la interfaz base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexación: División\n",
    "\n",
    "Nuestro documento cargado tiene más de 42,000 caracteres de longitud. Esto es demasiado largo para ajustarse en la ventana de contexto de muchos modelos. Incluso para aquellos modelos que podrían ajustar la publicación completa en su ventana de contexto, los modelos pueden tener dificultades para encontrar información en entradas muy largas.\n",
    "\n",
    "Para manejar esto, dividiremos el Documento en fragmentos para la inserción de vectores. Esto debería ayudarnos a recuperar solo las partes más relevantes del blog en tiempo de ejecución.\n",
    "\n",
    "En este caso, dividiremos nuestros documentos en fragmentos de 1000 caracteres con 200 caracteres de superposición entre fragmentos. La superposición ayuda a mitigar la posibilidad de separar una declaración de un contexto importante relacionado con ella. Utilizamos RecursiveCharacterTextSplitter, que dividirá recursivamente el documento utilizando separadores comunes como nuevas líneas hasta que cada fragmento tenga el tamaño adecuado. Este es el separador de texto recomendado para casos de uso de texto genérico.\n",
    "\n",
    "Configuramos add_start_index=True para que el índice de caracteres en el que comienza cada Documento dividido dentro del Documento inicial se conserve como atributo de metadatos \"start_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adentrémonos más\n",
    "\n",
    "**TextSplitter:** Objeto que divide una lista de Documentos en fragmentos más pequeños. Subclase de DocumentTransformers.\n",
    "- **Explora Separadores conscientes del contexto, que mantienen la ubicación (\"contexto\") de cada fragmento en el Documento original:**\n",
    "  - (Archivos Markdown)[https://python.langchain.com/docs/modules/data_connection/document_transformers/markdown_header_metadata]\n",
    "  - Código (py o js)\n",
    "  - Documentos científicos\n",
    "- **Interfaz:** Referencia de API para la interfaz base.\n",
    "\n",
    "**DocumentTransformer:** Objeto que realiza una transformación en una lista de Documentos.\n",
    "- **Documentación:** Documentación detallada sobre cómo utilizar DocumentTransformers.\n",
    "- **Integraciones:** Integraciones disponibles.\n",
    "- **Interfaz:** Referencia de API para la interfaz base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
