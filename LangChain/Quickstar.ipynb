{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install langchain -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo con LangChain\n",
    "LangChain permite desarrollar aplicaciones que conectan fuentes externas de datos y cálculos a LLMs (modelos de lenguaje grandes). En este inicio rápido, recorreremos algunas formas diferentes de hacerlo. Comenzaremos con una cadena LLM simple, que se basa únicamente en la información en la plantilla de indicaciones para responder. A continuación, construiremos una cadena de recuperación, que obtiene datos de una base de datos independiente y los pasa a la plantilla de indicaciones. Luego, agregaremos un historial de chat para crear una cadena de recuperación de conversación. Esto permite interactuar de manera similar a un chat con este LLM, para que recuerde preguntas anteriores. Finalmente, construiremos un agente, que utiliza un LLM para determinar si necesita o no obtener datos para responder preguntas. Abordaremos estos temas a un nivel alto, ¡pero hay muchos detalles en todo esto! Enlazaremos a documentos relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadena LLM\n",
    "Para esta guía de inicio, proporcionaremos dos opciones: utilizar OpenAI (un modelo popular disponible a través de API) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, necesitaremos importar el paquete de integración LangChain x OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayas instalado e inicializado el LLM de tu elección, ¡podemos intentar usarlo! Preguntémosle qué es LangSmith: esto es algo que no estaba presente en los datos de entrenamiento, así que no debería tener una respuesta muy buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith puede ayudar en las pruebas de varias maneras:\\n\\n1. Automatización de pruebas: Langsmith puede escribir scripts de pruebas automatizados para ejecutar pruebas repetitivas y garantizar la consistencia en los resultados. Esto ayuda a ahorrar tiempo y esfuerzo en las pruebas manuales.\\n\\n2. Generación automática de datos de prueba: Langsmith puede generar datos de prueba automáticamente, lo que facilita la creación de conjuntos de prueba exhaustivos y diversos. Esto ayuda a descubrir posibles problemas o errores en el software.\\n\\n3. Detección de errores y seguimiento de problemas: Langsmith puede monitorear y registrar los errores encontrados durante las pruebas. Esto ayuda a identificar rápidamente los problemas y seguir su resolución.\\n\\n4. Análisis y visualización de resultados de pruebas: Langsmith puede analizar los resultados de las pruebas y presentarlos de manera visualmente atractiva. Esto facilita la comprensión de los resultados de las pruebas y la toma de decisiones basadas en ellos.\\n\\n5. Integración con herramientas de gestión de pruebas: Langsmith puede integrarse con otras herramientas de gestión de pruebas, como sistemas de seguimiento de problemas o herramientas de gestión de proyectos. Esto permite una gestión más eficiente de las pruebas y una colaboración más fluida entre los miembros del equipo.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"¿Cómo puede ayudar Langsmith con las pruebas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos guiar su respuesta con una plantilla de indicaciones. Las plantillas de indicaciones se utilizan para convertir la entrada de usuario sin procesar en una entrada mejor para el LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un escritor de documentación técnica de clase mundial.\"),\n",
    "    (\"user\", \"{entrada}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos combinar estas en una cadena LLM simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocarlo y hacer la misma pregunta. Aún no conocerá la respuesta, pero debería responder en un tono más adecuado para un escritor técnico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith puede ayudar con las pruebas de diferentes maneras:\\n\\n1. Automatización de pruebas: Langsmith puede ayudar a desarrollar scripts de pruebas automatizadas para acelerar el proceso de prueba y garantizar la consistencia de los resultados. Esto puede incluir pruebas de unidad, pruebas de integración, pruebas de regresión, etc.\\n\\n2. Generación de datos de prueba: Langsmith puede generar datos de prueba realistas y relevantes para las pruebas. Esto puede incluir datos de prueba aleatorios, datos de prueba basados en patrones o datos de prueba específicos para cubrir casos de uso particulares.\\n\\n3. Creación de casos de prueba: Langsmith puede ayudar a crear casos de prueba detallados y exhaustivos para garantizar que todas las funcionalidades y requisitos del sistema se prueben adecuadamente. Esto incluye la identificación de escenarios de prueba, la definición de pasos de prueba y la especificación de resultados esperados.\\n\\n4. Documentación de pruebas: Langsmith puede ayudar a documentar los resultados de las pruebas, registrar problemas y generar informes de pruebas detallados. Esto facilita la comunicación entre los equipos de desarrollo y calidad y ayuda a rastrear y solucionar problemas de manera eficiente.\\n\\n5. Pruebas de rendimiento: Langsmith puede realizar pruebas de rendimiento para evaluar el rendimiento y la escalabilidad del sistema bajo diferentes cargas de trabajo. Esto implica medir y analizar métricas de rendimiento, identificar cuellos de botella y optimizar el rendimiento del sistema.\\n\\nEn resumen, Langsmith puede ser una herramienta valiosa para mejorar la eficiencia y la calidad de las pruebas de software al automatizar tareas repetitivas, generar datos de prueba relevantes, crear casos de prueba detallados, documentar los resultados y ayudar con las pruebas de rendimiento.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"entrada\": \"¿Cómo puede ayudar Langsmith con las pruebas?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de un modelo de chat (y, por lo tanto, de esta cadena) es un mensaje. Sin embargo, a menudo es mucho más conveniente trabajar con cadenas. Añadamos un simple analizador de salida para convertir el mensaje de chat en una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos agregar esto a la cadena anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocarlo y hacer la misma pregunta. La respuesta ahora será una cadena (en lugar de un ChatMessage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith puede ayudar en las pruebas de varias maneras:\\n\\n1. Documentación de casos de prueba: Como escritor de documentación técnica, puedo ayudar a crear una documentación detallada de los casos de prueba. Esto incluye describir los pasos necesarios para ejecutar las pruebas, los datos de entrada requeridos y los resultados esperados.\\n\\n2. Creación de guías de prueba: Puedo desarrollar guías de prueba que proporcionen instrucciones paso a paso sobre cómo ejecutar las pruebas correctamente. Estas guías pueden incluir capturas de pantalla, ejemplos de código y explicaciones detalladas para facilitar la comprensión y ejecución de las pruebas.\\n\\n3. Generación de informes de prueba: Puedo ayudar a generar informes de prueba claros y concisos que muestren los resultados de las pruebas realizadas. Estos informes pueden incluir métricas de rendimiento, errores encontrados, cobertura de código y cualquier otra información relevante.\\n\\n4. Revisión de documentación existente: Si ya tienes documentación de pruebas existente, puedo revisarla y mejorarla. Esto incluye corregir errores, mejorar la claridad y asegurarme de que la documentación cumpla con los estándares de calidad.\\n\\n5. Colaboración con equipos de desarrollo y QA: Trabajaré en estrecha colaboración con los equipos de desarrollo y aseguramiento de calidad (QA) para comprender completamente los requisitos y las características del software a probar. Esto asegurará que la documentación de prueba sea precisa y completa.\\n\\nEn resumen, como escritor de documentación técnica, puedo ayudar en la planificación, ejecución y documentación de pruebas, asegurando que todo el proceso de prueba esté bien documentado y sea fácil de seguir.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"entrada\": \"¿Cómo puede ayudar Langsmith con las pruebas?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profundizando​\n",
    "Ahora hemos configurado con éxito una cadena LLM básica. Solo hemos explorado los conceptos básicos de indicaciones, modelos y analizadores de salida; para obtener una comprensión más profunda de todo lo mencionado aquí, consulta esta sección de la documentación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadena de Recuperación\n",
    "Para responder adecuadamente a la pregunta original (\"¿cómo puede ayudar LangSmith con las pruebas?\"), necesitamos proporcionar contexto adicional al LLM. Podemos hacer esto mediante la recuperación. La recuperación es útil cuando tienes demasiados datos para pasar al LLM directamente. Luego puedes usar un recuperador para buscar solo las piezas más relevantes y pasarlas.\n",
    "\n",
    "En este proceso, buscaremos documentos relevantes desde un Recuperador y luego los pasaremos a la indicación. Un Recuperador puede basarse en cualquier cosa, como una tabla SQL, internet, etc., pero en este caso poblaremos una tienda de vectores y la utilizaremos como un recuperador. Para obtener más información sobre las tiendas de vectores, consulta esta documentación.\n",
    "\n",
    "Primero, necesitamos cargar los datos que queremos indexar. Para hacer esto, utilizaremos el WebBaseLoader. Esto requiere instalar BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %conda install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de eso, podemos importar y usar WebBaseLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, necesitamos indexarlo en una tienda de vectores. Esto requiere algunos componentes, en particular, un modelo de incrustación y una tienda de vectores.\n",
    "\n",
    "En cuanto a los modelos de incrustación, proporcionamos nuevamente ejemplos de acceso a través de OpenAI o mediante modelos locales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegúrate de tener instalado el paquete `langchain_openai` y las variables de entorno adecuadas configuradas (son las mismas que se necesitan para el LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos utilizar este modelo de incrustación para ingresar documentos en una tienda de vectores. Utilizaremos una tienda de vectores local y sencilla, FAISS, por simplicidad.\n",
    "\n",
    "Primero, necesitamos instalar los paquetes necesarios para ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %conda install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos construir nuestro índice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos estos datos indexados en una tienda de vectores, crearemos una cadena de recuperación. Esta cadena tomará una pregunta entrante, buscará documentos relevantes y luego pasará esos documentos junto con la pregunta original a un LLM y le pedirá que responda la pregunta original.\n",
    "\n",
    "Primero, configuremos la cadena que toma una pregunta y los documentos recuperados y genera una respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Responda la siguiente pregunta basándose únicamente en el contexto proporcionado:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith puede ayudar a visualizar los resultados de las pruebas.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"¿Cómo puede ayudar Langsmith con las pruebas?\",\n",
    "    \"context\": [Document(page_content=\"Langsmith puede permitirle visualizar los resultados de las pruebas\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, queremos que los documentos provengan primero del recuperador que acabamos de configurar. De esta manera, para una pregunta dada, podemos utilizar el recuperador para seleccionar dinámicamente los documentos más relevantes y pasarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocar esta cadena. Esto devuelve un diccionario: la respuesta del LLM se encuentra en la clave \"answer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith puede ayudar con las pruebas al proporcionar una forma fácil de visualizar los resultados de las llamadas a modelos de lenguaje (LLMs) y al permitir la modificación de las entradas de prueba en un \"playground\" para observar los cambios en la salida. También simplifica la carga de conjuntos de datos para probar cambios en los prompts o cadenas. Además, LangSmith ofrece evaluadores automáticos para analizar los resultados de las pruebas y permite la revisión y anotación manual de las ejecuciones a través de colas de anotaciones, lo que permite una revisión humana de calidad y confiabilidad.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"¿Cómo puede ayudar Langsmith con las pruebas?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadena de Recuperación de Conversación\n",
    "La cadena que hemos creado hasta ahora solo puede responder preguntas individuales. Uno de los principales tipos de aplicaciones de LLM que las personas están desarrollando son los chatbots. Entonces, ¿cómo convertimos esta cadena en una que pueda responder preguntas de seguimiento?\n",
    "\n",
    "Todavía podemos utilizar la función create_retrieval_chain, pero necesitamos cambiar dos cosas:\n",
    "\n",
    "1. El método de recuperación ahora no solo debe funcionar en la entrada más reciente, sino que debe tener en cuenta toda la historia.\n",
    "2. La cadena final del LLM también debe tener en cuenta toda la historia.\n",
    "\n",
    "Actualización de la Recuperación\n",
    "\n",
    "Para actualizar la recuperación, crearemos una nueva cadena. Esta cadena tomará la entrada más reciente (input) y el historial de la conversación (chat_history) y utilizará un LLM para generar una consulta de búsqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Dada la conversación anterior, genere una consulta de búsqueda para buscar información relevante para la conversación.\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar esto pasando una instancia en la que el usuario hace una pregunta de seguimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a “Share” button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, it’s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You don’t have any examples to benchmark your changes against.LangSmith addresses this problem by including an “Add to Dataset” button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"¿Puede LangSmith ayudarme a probar mis solicitudes de LLM?\"), AIMessage(content=\"¡Si!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Dime como\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deberías ver que esto devuelve documentos sobre pruebas en LangSmith. Esto se debe a que el LLM generó una nueva consulta, combinando el historial del chat con la pregunta de seguimiento.\n",
    "\n",
    "Ahora que tenemos este nuevo recuperador, podemos crear una nueva cadena para continuar la conversación teniendo en cuenta estos documentos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responda las preguntas del usuario según el siguiente contexto:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='¿Puede LangSmith ayudarme a probar mis solicitudes de LLM?'),\n",
       "  AIMessage(content='¡SI!')],\n",
       " 'input': 'Dime como',\n",
       " 'context': [Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a “Share” button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, it’s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You don’t have any examples to benchmark your changes against.LangSmith addresses this problem by including an “Add to Dataset” button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith puede ayudarte a probar tus solicitudes de LLM de las siguientes maneras:\\n\\n1. Visualización de entradas/salidas exactas: LangSmith proporciona una visualización clara de las entradas y salidas exactas de todas las llamadas de LLM. Esto te permite comprender fácilmente cómo se están procesando tus solicitudes y verificar si los resultados son los esperados.\\n\\n2. Modificación de la solicitud en tiempo real: Cuando encuentres una salida incorrecta, puedes utilizar la función \"Open in Playground\" de LangSmith para acceder a un entorno de prueba. Aquí, puedes modificar la solicitud y ejecutarla nuevamente para observar los cambios en la salida. Esto te permite realizar pruebas iterativas y experimentar con diferentes modificaciones para obtener los resultados deseados.\\n\\n3. Seguimiento de eventos y latencia: LangSmith ofrece un seguimiento detallado de los eventos y la latencia de cada paso de tu solicitud. Esto te permite identificar y solucionar cualquier cuello de botella o problema de rendimiento en tu cadena de solicitudes.\\n\\n4. Recopilación de ejemplos: Puedes utilizar LangSmith para recopilar ejemplos de entrada/salida de tus solicitudes. Estos ejemplos pueden ser útiles para probar y evaluar futuras versiones de tu cadena de solicitudes, así como para construir conjuntos de datos de evaluación.\\n\\nEn resumen, LangSmith te brinda las herramientas necesarias para probar y depurar tus solicitudes de LLM de manera efectiva, lo que te ayuda a garantizar la calidad y el rendimiento de tus aplicaciones.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"¿Puede LangSmith ayudarme a probar mis solicitudes de LLM?\"), AIMessage(content=\"¡SI!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Dime como\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que esto da una respuesta coherente: ¡hemos convertido con éxito nuestra cadena de recuperación en un chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente\n",
    "Hasta ahora, hemos creado ejemplos de cadenas, donde cada paso se conoce de antemano. Lo último que crearemos es un agente, donde el LLM decide qué pasos tomar.\n",
    "\n",
    "NOTA: para este ejemplo, solo mostraremos cómo crear un agente utilizando modelos de OpenAI, ya que los modelos locales aún no son lo suficientemente confiables.\n",
    "\n",
    "Una de las primeras cosas que debes hacer al construir un agente es decidir a qué herramientas debería tener acceso. Para este ejemplo, daremos al agente acceso a dos herramientas:\n",
    "\n",
    "1. El recuperador que acabamos de crear. Esto le permitirá responder fácilmente preguntas sobre LangSmith.\n",
    "2. Una herramienta de búsqueda. Esto le permitirá responder fácilmente preguntas que requieran información actualizada.\n",
    "\n",
    "Primero, configuremos una herramienta para el recuperador que acabamos de crear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Buscar información sobre LangSmith. ¡Para cualquier pregunta sobre LangSmith, debes utilizar esta herramienta!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La herramienta de búsqueda que utilizaremos es Tavily. Esto requerirá una clave API (tienen un generoso nivel gratuito). Después de crearlo en su plataforma, debes configurarlo como una variable de entorno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualizamos el .env con la clave de tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear una lista de las herramientas con las que queremos trabajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las herramientas, podemos crear un agente para usarlas. Repasaremos esto rápidamente; para profundizar en lo que está sucediendo exactamente, consulte la documentación de introducción del agente. \n",
    "\n",
    "Instale primero el HUB langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchainhub in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (0.1.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchainhub) (2.31.0.20240125)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usarlo para obtener un mensaje predefinido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora podemos invocar al agente y ver cómo responde! Podemos hacerle preguntas sobre LangSmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith puede ayudar con las pruebas de varias maneras:\n",
      "\n",
      "1. Generación automática de casos de prueba: LangSmith puede generar automáticamente casos de prueba para diferentes escenarios y condiciones. Esto ahorra tiempo y esfuerzo al crear casos de prueba manualmente.\n",
      "\n",
      "2. Ejecución de pruebas automatizadas: LangSmith puede ejecutar pruebas automatizadas para verificar el funcionamiento correcto de una aplicación o sistema. Esto incluye la ejecución de pruebas unitarias, pruebas de integración y pruebas de regresión.\n",
      "\n",
      "3. Análisis de cobertura de código: LangSmith puede analizar la cobertura de código de las pruebas para identificar áreas que no están siendo probadas adecuadamente. Esto ayuda a garantizar una cobertura completa y efectiva de las pruebas.\n",
      "\n",
      "4. Generación de informes de pruebas: LangSmith puede generar informes detallados sobre los resultados de las pruebas, incluyendo información sobre los casos de prueba ejecutados, los errores encontrados y las métricas de cobertura de código. Estos informes son útiles para el seguimiento y la documentación de las pruebas.\n",
      "\n",
      "En resumen, LangSmith puede automatizar y optimizar el proceso de pruebas, lo que ayuda a mejorar la calidad y la eficiencia de las pruebas en el desarrollo de software.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¿Cómo puede ayudar Langsmith con las pruebas?',\n",
       " 'output': 'LangSmith puede ayudar con las pruebas de varias maneras:\\n\\n1. Generación automática de casos de prueba: LangSmith puede generar automáticamente casos de prueba para diferentes escenarios y condiciones. Esto ahorra tiempo y esfuerzo al crear casos de prueba manualmente.\\n\\n2. Ejecución de pruebas automatizadas: LangSmith puede ejecutar pruebas automatizadas para verificar el funcionamiento correcto de una aplicación o sistema. Esto incluye la ejecución de pruebas unitarias, pruebas de integración y pruebas de regresión.\\n\\n3. Análisis de cobertura de código: LangSmith puede analizar la cobertura de código de las pruebas para identificar áreas que no están siendo probadas adecuadamente. Esto ayuda a garantizar una cobertura completa y efectiva de las pruebas.\\n\\n4. Generación de informes de pruebas: LangSmith puede generar informes detallados sobre los resultados de las pruebas, incluyendo información sobre los casos de prueba ejecutados, los errores encontrados y las métricas de cobertura de código. Estos informes son útiles para el seguimiento y la documentación de las pruebas.\\n\\nEn resumen, LangSmith puede automatizar y optimizar el proceso de pruebas, lo que ayuda a mejorar la calidad y la eficiencia de las pruebas en el desarrollo de software.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"¿Cómo puede ayudar Langsmith con las pruebas?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos preguntarle sobre el clima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'clima en NY'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://www.expreso.ec/actualidad/mundo/nueva-york-alista-tormenta-dejar-20-centimetros-nieve-189453.html', 'content': 'Clima extremo El gran negocio del nuevo aeropuerto de Guayaquil 11 febrero, 2024  Nueva York: alerta por tormenta invernal con lluvias y potenciales inundaciones  Mundo Nueva York se alista para tormenta que puede dejar hasta 20 centímetros de nieve  El cabildo local ordenó clases virtuales para este martes 13 de febrero de 2024. Se esperan fuertes vientos4 hours ago — 4 hours agoNueva York se alista para tormenta que puede dejar hasta 20 centímetros de nieve. El cabildo local ordenó clases virtuales para este martes 13\\xa0...'}]\u001b[0m\u001b[32;1m\u001b[1;3mSegún los resultados de búsqueda, en Nueva York se espera una tormenta invernal con lluvias y potenciales inundaciones. También se espera que la tormenta deje hasta 20 centímetros de nieve. El cabildo local ha ordenado clases virtuales para el martes 13 de febrero de 2024 debido a esta situación.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '¿Cuál es el clima en NY?',\n",
       " 'output': 'Según los resultados de búsqueda, en Nueva York se espera una tormenta invernal con lluvias y potenciales inundaciones. También se espera que la tormenta deje hasta 20 centímetros de nieve. El cabildo local ha ordenado clases virtuales para el martes 13 de febrero de 2024 debido a esta situación.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"¿Cuál es el clima en NY?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPara probar tus solicitudes de LLM (Lenguaje de Modelado de Lenguajes) con LangSmith, puedes seguir estos pasos:\n",
      "\n",
      "1. Formula tu solicitud de LLM en un formato compatible con LangSmith. Puedes utilizar el lenguaje de programación que prefieras para escribir tu solicitud, como Python, JavaScript, o cualquier otro lenguaje compatible con la API de LangSmith.\n",
      "\n",
      "2. Utiliza la función `langsmith_search` de LangSmith para enviar tu solicitud y obtener los resultados. Esta función acepta un objeto con la propiedad `query`, que debe ser tu solicitud de LLM en formato de texto.\n",
      "\n",
      "3. Analiza los resultados devueltos por la función `langsmith_search`. Puedes acceder a la información relevante de los resultados para evaluar la precisión y calidad de tu solicitud de LLM.\n",
      "\n",
      "Aquí tienes un ejemplo de cómo podrías utilizar la función `langsmith_search` en JavaScript:\n",
      "\n",
      "```javascript\n",
      "const query = \"Mi solicitud de LLM\";\n",
      "const results = langsmith_search({ query });\n",
      "\n",
      "// Analiza los resultados devueltos\n",
      "console.log(results);\n",
      "```\n",
      "\n",
      "Recuerda que LangSmith es una herramienta de búsqueda de información, por lo que los resultados que obtengas dependerán de la calidad de tu solicitud de LLM y de la información disponible en la base de datos de LangSmith.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='¿Puede LangSmith ayudarme a probar mis solicitudes de LLM?'),\n",
       "  AIMessage(content='¡Si!')],\n",
       " 'input': 'Dime como',\n",
       " 'output': 'Para probar tus solicitudes de LLM (Lenguaje de Modelado de Lenguajes) con LangSmith, puedes seguir estos pasos:\\n\\n1. Formula tu solicitud de LLM en un formato compatible con LangSmith. Puedes utilizar el lenguaje de programación que prefieras para escribir tu solicitud, como Python, JavaScript, o cualquier otro lenguaje compatible con la API de LangSmith.\\n\\n2. Utiliza la función `langsmith_search` de LangSmith para enviar tu solicitud y obtener los resultados. Esta función acepta un objeto con la propiedad `query`, que debe ser tu solicitud de LLM en formato de texto.\\n\\n3. Analiza los resultados devueltos por la función `langsmith_search`. Puedes acceder a la información relevante de los resultados para evaluar la precisión y calidad de tu solicitud de LLM.\\n\\nAquí tienes un ejemplo de cómo podrías utilizar la función `langsmith_search` en JavaScript:\\n\\n```javascript\\nconst query = \"Mi solicitud de LLM\";\\nconst results = langsmith_search({ query });\\n\\n// Analiza los resultados devueltos\\nconsole.log(results);\\n```\\n\\nRecuerda que LangSmith es una herramienta de búsqueda de información, por lo que los resultados que obtengas dependerán de la calidad de tu solicitud de LLM y de la información disponible en la base de datos de LangSmith.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"¿Puede LangSmith ayudarme a probar mis solicitudes de LLM?\"), AIMessage(content=\"¡Si!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Dime como\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sirviendo con LangServe\n",
    "Ahora que hemos construido una aplicación, necesitamos ponerla en funcionamiento. Ahí es donde entra LangServe. LangServe ayuda a los desarrolladores a implementar cadenas de LangChain como una API REST. No es necesario utilizar LangServe para usar LangChain, pero en esta guía mostraremos cómo puedes implementar tu aplicación con LangServe.\n",
    "\n",
    "Mientras que la primera parte de esta guía estaba destinada a ejecutarse en un cuaderno Jupyter, ahora saldremos de eso. Crearemos un archivo Python y luego interactuaremos con él desde la línea de comandos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langserve[all] in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (0.0.41)\n",
      "Requirement already satisfied: fastapi<1,>=0.90.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (0.109.2)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (0.26.0)\n",
      "Requirement already satisfied: httpx-sse>=0.3.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (0.4.0)\n",
      "Requirement already satisfied: langchain>=0.0.333 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (0.1.6)\n",
      "Requirement already satisfied: orjson>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (3.9.13)\n",
      "Requirement already satisfied: pydantic>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (1.10.12)\n",
      "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langserve[all]) (1.8.2)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (0.36.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (4.9.0)\n",
      "Requirement already satisfied: anyio in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (4.2.0)\n",
      "Requirement already satisfied: certifi in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->langserve[all]) (0.14.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyYAML>=5.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.5.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.19)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.1.22)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (1.26.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (8.2.3)\n",
      "Requirement already satisfied: uvicorn in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from sse-starlette<2.0.0,>=1.3.0->langserve[all]) (0.27.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (3.20.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.333->langserve[all]) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.22->langchain>=0.0.333->langserve[all]) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.333->langserve[all]) (3.0.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from uvicorn->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/g-999/anaconda3/envs/LangChain/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Servidor \n",
    "Para crear un servidor para nuestra aplicación, crearemos un archivo [serve.py](http://localhost:8889/lab/tree/Documentos/GitClone/CookBook/LangChain/serve.py). \n",
    "Esto contendrá nuestra lógica para servir nuestra aplicación. \n",
    "Consta de tres cosas: \n",
    "1. La definición de nuestra cadena que acabamos de crear arriba\n",
    "2. Nuestra aplicación FastAPI\n",
    "3. Una definición de una ruta desde la cual servir a la cadena, que se hace con langserve.add_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deberíamos ver nuestra [cadena](http://localhost:8889/lab/tree/Documentos/GitClone/CookBook/LangChain/serve.py) siendo atendida en localhost:8000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground \n",
    "Cada servicio LangServe viene con una interfaz de usuario integrada simple para configurar e invocar la aplicación con salida de transmisión y visibilidad de los pasos intermedios. \n",
    "Dirígete a http://localhost:8000/agent/playground/ ¡para probarlo! Pase la misma pregunta que antes: \"¿Cómo puede ayudar Langsmith con las pruebas?\" - y debería responder igual que antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliente \n",
    "Ahora configuremos un cliente para interactuar mediante programación con nuestro servicio. Podemos hacer esto fácilmente con [langserve.RemoteRunnable](/docs/langserve#client) Al usar esto, podemos interactuar con la cadena servida como si se estuviera ejecutando en el lado del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'LangSmith puede ayudar con las pruebas de varias maneras:\\n\\n1. Generación automática de casos de prueba: LangSmith puede generar automáticamente casos de prueba para diferentes escenarios y condiciones. Esto puede ahorrar tiempo y esfuerzo en la creación manual de casos de prueba.\\n\\n2. Ejecución de pruebas automatizadas: LangSmith puede ejecutar pruebas automatizadas para verificar el funcionamiento correcto de un sistema o aplicación. Esto puede incluir pruebas de regresión, pruebas de integración y pruebas de rendimiento.\\n\\n3. Análisis de resultados de pruebas: LangSmith puede analizar los resultados de las pruebas y proporcionar informes detallados sobre los errores encontrados, la cobertura de código y otros aspectos importantes de las pruebas.\\n\\n4. Optimización de pruebas: LangSmith puede ayudar a optimizar las pruebas identificando áreas de código que no están siendo probadas adecuadamente y sugiriendo mejoras en la estrategia de pruebas.\\n\\n5. Integración con herramientas de gestión de pruebas: LangSmith puede integrarse con herramientas de gestión de pruebas existentes para facilitar el seguimiento y la gestión de las pruebas.\\n\\nEn resumen, LangSmith puede ayudar a mejorar la eficiencia y la calidad de las pruebas al automatizar tareas repetitivas, proporcionar análisis detallados y optimizar la estrategia de pruebas.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
    "remote_chain.invoke({\n",
    "    \"input\": \"¿Cómo puede ayudar Langsmith con las pruebas?\",\n",
    "    \"chat_history\": []  # Providing an empty list as this is the first call\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
